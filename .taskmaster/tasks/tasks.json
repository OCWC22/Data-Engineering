{
  "tasks": [
    {
      "id": 1,
      "title": "Configure AWS S3 Integration",
      "description": "Set up a local, S3-compatible development environment using MinIO running in Docker, and configure the platform to use it for storage.",
      "status": "done",
      "dependencies": [],
      "priority": "high",
      "details": "1. Create a Docker Compose file for MinIO service\n2. Configure MinIO with default credentials and expose ports 9000 (API) and 9001 (Console)\n3. Create scripts/instructions for starting the service and initializing a 'neuralake-bucket'\n4. Update the `neuralake` library configuration to accept custom S3 endpoint URLs\n5. Implement storage abstraction layer that can work with both local and S3-compatible backends\n6. Add proper error handling for network and permission issues",
      "testStrategy": "1. Verify MinIO Docker container starts correctly and is accessible\n2. Test bucket creation using AWS CLI configured for MinIO endpoint\n3. Unit tests for S3 backend operations with MinIO\n4. Test writing and reading a small Parquet file to 'neuralake-bucket' using delta-rs or pyarrow\n5. Test error handling for connection issues and permissions\n6. Verify the same code works with both MinIO and actual AWS S3 by just changing the endpoint URL",
      "subtasks": [
        {
          "id": "1.1",
          "title": "Create Docker Compose file for MinIO",
          "status": "done",
          "description": "Create a Docker Compose file that defines and runs the MinIO service with appropriate configuration."
        },
        {
          "id": "1.2",
          "title": "Write setup scripts for MinIO",
          "status": "done",
          "description": "Create scripts or documentation for starting MinIO and initializing the 'neuralake-bucket' using AWS CLI."
        },
        {
          "id": "1.3",
          "title": "Update neuralake library for S3 endpoint",
          "status": "done",
          "description": "Modify library configuration to accept an S3 endpoint URL for connecting to local MinIO."
        },
        {
          "id": "1.4",
          "title": "Test S3 integration",
          "status": "done",
          "description": "Write and run a test to confirm that a small Parquet file can be successfully written to and read from the local MinIO bucket."
        }
      ]
    },
    {
      "id": 2,
      "title": "Integrate Code Quality Tooling",
      "description": "Add ruff as a linter and formatter to pyproject.toml dev dependencies, configure it, run it across the codebase to fix issues, and update README.md with usage instructions.",
      "status": "done",
      "dependencies": [
        1
      ],
      "priority": "high",
      "details": "1. Add `ruff` to the `[tool.poetry.group.dev.dependencies]` section in `pyproject.toml`\n2. Create a `pyproject.toml` configuration section for ruff with appropriate settings\n3. Run `ruff check` across the entire codebase to identify issues\n4. Run `ruff format` to automatically fix formatting issues\n5. Address any remaining linting issues that require manual fixes\n6. Update the main `README.md` with a \"Code Quality\" section explaining ruff usage\n7. Add ruff commands to any existing development scripts or documentation",
      "testStrategy": "1. Verify ruff installation and configuration\n2. Test that `ruff check` runs without errors after fixes\n3. Test that `ruff format` produces consistent formatting\n4. Verify all Python files pass linting checks\n5. Test that development workflow documentation is clear and accurate\n6. Ensure ruff integrates properly with existing development tools",
      "subtasks": [
        {
          "id": 1,
          "title": "Add ruff to pyproject.toml dev dependencies",
          "description": "Add ruff as a development dependency in the Poetry configuration file with specific version and install it",
          "details": "**Files to modify:**\n- `neuralake/pyproject.toml`\n\n**Exact steps:**\n1. Navigate to `neuralake/` directory: `cd neuralake/`\n2. Add ruff to dev dependencies by editing `pyproject.toml`:\n   ```toml\n   [tool.poetry.group.dev.dependencies]\n   ruff = \"^0.1.9\"\n   pytest = \"^7.4.3\"\n   pytest-cov = \"^4.1.0\"\n   ```\n3. Install the new dependency: `poetry install`\n4. Verify installation: `poetry run ruff --version`\n5. Create initial ruff configuration in pyproject.toml\n\n**Expected output:** \n- ruff version should be displayed\n- `poetry.lock` file updated with ruff dependencies\n\n**Error handling:**\n- If poetry install fails, run `poetry update` first\n- If ruff not found, ensure you're in poetry shell: `poetry shell`",
          "status": "done",
          "dependencies": [],
          "parentTaskId": 2
        },
        {
          "id": 2,
          "title": "Configure comprehensive ruff settings in pyproject.toml",
          "description": "Set up detailed ruff configuration with specific rules, exclusions, and formatting options for production-ready code quality",
          "details": "**Files to modify:**\n- `neuralake/pyproject.toml`\n\n**Exact configuration to add:**\n```toml\n[tool.ruff]\n# Set the maximum line length to 88 (black default)\nline-length = 88\ntarget-version = \"py311\"\n\n# Enable specific rule groups\nselect = [\n    \"E\",      # pycodestyle errors\n    \"W\",      # pycodestyle warnings  \n    \"F\",      # pyflakes\n    \"I\",      # isort\n    \"B\",      # flake8-bugbear\n    \"C4\",     # flake8-comprehensions\n    \"UP\",     # pyupgrade\n    \"ARG\",    # flake8-unused-arguments\n    \"SIM\",    # flake8-simplify\n    \"TCH\",    # flake8-type-checking\n    \"PTH\",    # flake8-use-pathlib\n]\n\n# Never enforce these rules\nignore = [\n    \"E501\",   # Line too long (handled by formatter)\n    \"B008\",   # Do not perform function calls in argument defaults\n    \"B904\",   # Allow raise without from\n]\n\n# Exclude specific directories\nexclude = [\n    \".git\",\n    \".mypy_cache\",\n    \".ruff_cache\",\n    \".venv\",\n    \"__pycache__\",\n    \"build\",\n    \"dist\",\n    \"docs/build/\",\n]\n\n[tool.ruff.format]\n# Use double quotes for strings\nquote-style = \"double\"\n# Prefer Unix line endings\nline-ending = \"lf\"\n\n[tool.ruff.lint.isort]\n# Group imports by type\nforce-single-line = false\nforce-sort-within-sections = true\nknown-first-party = [\"neuralake\"]\n```\n\n**Commands to test configuration:**\n1. `cd neuralake/`\n2. `poetry run ruff check .` (should show current issues)\n3. `poetry run ruff check --select F .` (check only critical errors)\n4. `poetry run ruff format --check .` (check formatting without changing)\n\n**Validation steps:**\n- Configuration should load without errors\n- Rules should apply to Python files in src/ and scripts/\n- Exclusions should work for __pycache__ directories",
          "status": "done",
          "dependencies": [],
          "parentTaskId": 2
        },
        {
          "id": 3,
          "title": "Run ruff across codebase and systematically fix all issues",
          "description": "Execute comprehensive ruff analysis and fix all linting and formatting issues in the existing codebase",
          "details": "**Commands to execute in order:**\n\n1. **Navigate to project directory:**\n   ```bash\n   cd neuralake/\n   ```\n\n2. **Initial assessment - check all issues:**\n   ```bash\n   poetry run ruff check . --output-format=github\n   ```\n\n3. **Auto-fix safe issues:**\n   ```bash\n   poetry run ruff check . --fix\n   ```\n\n4. **Format all Python files:**\n   ```bash\n   poetry run ruff format .\n   ```\n\n5. **Check remaining issues after auto-fix:**\n   ```bash\n   poetry run ruff check . --diff\n   ```\n\n6. **Fix specific file types systematically:**\n   ```bash\n   # Check core source files\n   poetry run ruff check src/ --select F,E\n   \n   # Check scripts directory  \n   poetry run ruff check scripts/ --select F,E\n   \n   # Check any remaining Python files\n   poetry run ruff check *.py --select F,E\n   ```\n\n**Files that will likely need manual fixes:**\n- `src/config.py` - import organization\n- `src/my_tables.py` - unused imports, line length\n- `src/my_catalog.py` - import order\n- `src/query_data.py` - formatting issues\n- `scripts/*.py` - various formatting issues\n\n**Expected manual fixes needed:**\n- Remove unused imports (F401 errors)\n- Fix line length issues (E501 - if not auto-formatted)\n- Organize imports per isort rules (I001 errors)\n- Fix undefined variables (F821 errors)\n- Remove unused variables (F841 errors)\n\n**Validation commands:**\n```bash\n# Final check - should show zero issues\npoetry run ruff check .\n\n# Confirm formatting is correct\npoetry run ruff format --check .\n```\n\n**Error handling:**\n- If syntax errors appear, fix Python syntax first\n- If import errors occur, ensure all dependencies are installed: `poetry install`\n- For persistent issues, check specific rules with: `poetry run ruff rule <RULE_CODE>`",
          "status": "done",
          "dependencies": [],
          "parentTaskId": 2
        },
        {
          "id": 4,
          "title": "Create development scripts and update README with comprehensive code quality documentation",
          "description": "Add convenience scripts for code quality checks and comprehensive README documentation with examples and troubleshooting",
          "details": "**Files to create/modify:**\n\n1. **Create `neuralake/scripts/lint.py`:**\n```python\n#!/usr/bin/env python3\n\"\"\"\nDevelopment script for running code quality checks.\nUsage: python scripts/lint.py [--fix] [--format-only] [--check-only]\n\"\"\"\nimport subprocess\nimport sys\nfrom pathlib import Path\n\ndef run_command(cmd: list[str], description: str) -> bool:\n    \"\"\"Run a command and return success status.\"\"\"\n    print(f\"ðŸ” {description}\")\n    result = subprocess.run(cmd, capture_output=True, text=True)\n    \n    if result.returncode == 0:\n        print(f\"âœ… {description} - PASSED\")\n        if result.stdout.strip():\n            print(result.stdout)\n        return True\n    else:\n        print(f\"âŒ {description} - FAILED\")\n        if result.stderr.strip():\n            print(result.stderr)\n        if result.stdout.strip():\n            print(result.stdout)\n        return False\n\ndef main():\n    \"\"\"Main lint script.\"\"\"\n    fix_mode = \"--fix\" in sys.argv\n    format_only = \"--format-only\" in sys.argv\n    check_only = \"--check-only\" in sys.argv\n    \n    # Change to project root\n    os.chdir(Path(__file__).parent.parent)\n    \n    success = True\n    \n    if not check_only:\n        if fix_mode:\n            success &= run_command(\n                [\"poetry\", \"run\", \"ruff\", \"check\", \".\", \"--fix\"],\n                \"Auto-fixing ruff issues\"\n            )\n        \n        success &= run_command(\n            [\"poetry\", \"run\", \"ruff\", \"format\", \".\"],\n            \"Formatting code with ruff\"\n        )\n    \n    if not format_only:\n        success &= run_command(\n            [\"poetry\", \"run\", \"ruff\", \"check\", \".\"],\n            \"Checking code with ruff\"\n        )\n        \n        success &= run_command(\n            [\"poetry\", \"run\", \"ruff\", \"format\", \"--check\", \".\"],\n            \"Checking code formatting\"\n        )\n    \n    if success:\n        print(\"\\nðŸŽ‰ All code quality checks passed!\")\n        sys.exit(0)\n    else:\n        print(\"\\nðŸ’¥ Some checks failed. Please fix the issues above.\")\n        sys.exit(1)\n\nif __name__ == \"__main__\":\n    main()\n```\n\n2. **Update `neuralake/README.md` - Add Code Quality section:**\n```markdown\n## Code Quality\n\nThis project uses [ruff](https://github.com/astral-sh/ruff) for linting and formatting to ensure consistent, high-quality code.\n\n### Quick Commands\n\n```bash\n# Run all quality checks\npython scripts/lint.py\n\n# Auto-fix issues and format code\npython scripts/lint.py --fix\n\n# Only format code (no linting)\npython scripts/lint.py --format-only\n\n# Only check (no formatting)\npython scripts/lint.py --check-only\n```\n\n### Manual Commands\n\n```bash\n# Check for issues\npoetry run ruff check .\n\n# Auto-fix safe issues\npoetry run ruff check . --fix\n\n# Format all Python files\npoetry run ruff format .\n\n# Check formatting without changing files\npoetry run ruff format --check .\n```\n\n### Configuration\n\nRuff configuration is in `pyproject.toml` under `[tool.ruff]`. Key settings:\n\n- **Line length:** 88 characters (Black default)\n- **Target Python:** 3.11+\n- **Enabled rules:** pycodestyle, pyflakes, isort, flake8-bugbear, and more\n- **Import sorting:** Groups imports with neuralake as first-party\n\n### Pre-commit Workflow\n\nBefore committing code:\n\n1. Run `python scripts/lint.py --fix` to auto-fix issues\n2. Review and commit the changes\n3. Ensure `python scripts/lint.py` passes with no errors\n\n### Troubleshooting\n\n**Import errors:**\n```bash\n# Ensure all dependencies are installed\npoetry install\n```\n\n**Syntax errors:**\n- Fix Python syntax issues first before running ruff\n\n**Persistent formatting issues:**\n```bash\n# Check specific file\npoetry run ruff check src/my_tables.py --verbose\n\n# Get help on specific rule\npoetry run ruff rule F401\n```\n\n### IDE Integration\n\n**VS Code:**\nInstall the \"Ruff\" extension for real-time linting and formatting.\n\n**PyCharm:**\nConfigure ruff as external tool or use the ruff plugin.\n```\n\n3. **Create `neuralake/Makefile` for convenience:**\n```makefile\n.PHONY: lint format check install test help\n\nhelp:  ## Show this help\n\t@grep -E '^[a-zA-Z_-]+:.*?## .*$$' $(MAKEFILE_LIST) | sort | awk 'BEGIN {FS = \":.*?## \"}; {printf \"\\033[36m%-20s\\033[0m %s\\n\", $$1, $$2}'\n\ninstall:  ## Install dependencies\n\tpoetry install\n\nlint:  ## Run linting and formatting\n\tpython scripts/lint.py --fix\n\ncheck:  ## Check code quality without fixing\n\tpython scripts/lint.py\n\nformat:  ## Format code only\n\tpython scripts/lint.py --format-only\n\ntest:  ## Run tests (when implemented)\n\tpoetry run pytest\n\nclean:  ## Clean cache files\n\tfind . -type d -name \"__pycache__\" -delete\n\tfind . -type d -name \".ruff_cache\" -delete\n\tfind . -type f -name \"*.pyc\" -delete\n```\n\n**Commands to test setup:**\n```bash\ncd neuralake/\nchmod +x scripts/lint.py\npython scripts/lint.py --check-only\nmake help\nmake check\n```",
          "status": "done",
          "dependencies": [],
          "parentTaskId": 2
        }
      ]
    },
    {
      "id": 3,
      "title": "Implement CI/CD Pipeline",
      "description": "Create a GitHub Actions workflow that triggers on push/pull-request to main, installs dependencies via Poetry, runs code quality tools, and executes the production verification test suite.",
      "status": "done",
      "dependencies": [
        2
      ],
      "priority": "high",
      "details": "1. Create `.github/workflows/ci.yml` file with comprehensive CI/CD pipeline\n2. Configure workflow to trigger on push and pull requests to main branch\n3. Set up Poetry installation and dependency caching for faster builds\n4. Add step to run ruff linting and formatting checks\n5. Execute `production_verification.py` test suite in both `local` and `production` modes\n6. Add test result reporting and failure notifications\n7. Configure environment variables for CI execution\n8. Add badge to README showing build status",
      "testStrategy": "1. Test workflow triggers on push and pull request events\n2. Verify Poetry installation and dependency caching works correctly\n3. Test that ruff checks fail the build when code quality issues exist\n4. Verify production_verification.py runs successfully in both modes\n5. Test that build fails appropriately when tests fail\n6. Verify environment variable handling in CI environment\n7. Test badge integration and status reporting",
      "subtasks": [
        {
          "id": "3.1",
          "title": "Create GitHub Actions workflow file",
          "status": "done",
          "description": "Set up the .github/workflows/ci.yml file with Poetry and Python configuration"
        },
        {
          "id": "3.2",
          "title": "Configure code quality checks in CI",
          "status": "done",
          "description": "Add ruff linting and formatting verification steps to the workflow"
        },
        {
          "id": "3.3",
          "title": "Integrate production verification tests",
          "status": "done",
          "description": "Add steps to run the production_verification.py suite in both local and production modes"
        },
        {
          "id": "3.4",
          "title": "Add build status badge to README",
          "status": "done",
          "description": "Update README.md with GitHub Actions build status badge"
        },
        {
          "id": 4.4,
          "title": "Create GitHub Actions workflow structure",
          "description": "Set up the GitHub Actions directory structure and create the main CI workflow file with complete job definitions",
          "details": "**Directory structure to create:**\n```\n.github/\nâ”œâ”€â”€ workflows/\nâ”‚   â”œâ”€â”€ ci.yml\nâ”‚   â””â”€â”€ release.yml (future)\nâ””â”€â”€ dependabot.yml (future)\n```\n\n**Create `.github/workflows/ci.yml`:**\n```yaml\nname: CI Pipeline\n\non:\n  push:\n    branches: [ main, develop ]\n  pull_request:\n    branches: [ main ]\n\nenv:\n  PYTHON_VERSION: \"3.11\"\n\njobs:\n  lint-and-format:\n    name: Code Quality Checks\n    runs-on: ubuntu-latest\n    defaults:\n      run:\n        working-directory: ./neuralake\n    \n    steps:\n    - name: Checkout code\n      uses: actions/checkout@v4\n    \n    - name: Set up Python\n      uses: actions/setup-python@v4\n      with:\n        python-version: ${{ env.PYTHON_VERSION }}\n    \n    - name: Install Poetry\n      uses: snok/install-poetry@v1\n      with:\n        version: 1.6.1\n        virtualenvs-create: true\n        virtualenvs-in-project: true\n    \n    - name: Cache Poetry dependencies\n      uses: actions/cache@v3\n      with:\n        path: neuralake/.venv\n        key: ${{ runner.os }}-poetry-${{ hashFiles('neuralake/poetry.lock') }}\n        restore-keys: |\n          ${{ runner.os }}-poetry-\n    \n    - name: Install dependencies\n      run: poetry install\n    \n    - name: Run ruff linting\n      run: poetry run ruff check . --output-format=github\n    \n    - name: Run ruff formatting check\n      run: poetry run ruff format --check .\n    \n    - name: Check import sorting\n      run: poetry run ruff check . --select I --diff\n\n  test-local:\n    name: Test Suite (Local Mode)\n    runs-on: ubuntu-latest\n    needs: lint-and-format\n    defaults:\n      run:\n        working-directory: ./neuralake\n    \n    services:\n      minio:\n        image: minio/minio:latest\n        ports:\n          - 9000:9000\n          - 9001:9001\n        env:\n          MINIO_ROOT_USER: minioadmin\n          MINIO_ROOT_PASSWORD: minioadmin\n        options: >-\n          --health-cmd \"curl -f http://localhost:9000/minio/health/live\"\n          --health-interval 30s\n          --health-timeout 20s\n          --health-retries 3\n        command: server /data --console-address \":9001\"\n    \n    steps:\n    - name: Checkout code\n      uses: actions/checkout@v4\n    \n    - name: Set up Python\n      uses: actions/setup-python@v4\n      with:\n        python-version: ${{ env.PYTHON_VERSION }}\n    \n    - name: Install Poetry\n      uses: snok/install-poetry@v1\n      with:\n        version: 1.6.1\n        virtualenvs-create: true\n        virtualenvs-in-project: true\n    \n    - name: Install dependencies\n      run: poetry install\n    \n    - name: Wait for MinIO\n      run: |\n        timeout 60s bash -c 'until curl -f http://localhost:9000/minio/health/live; do sleep 2; done'\n    \n    - name: Set up MinIO bucket\n      run: |\n        pip install minio\n        python -c \"\n        from minio import Minio\n        client = Minio('localhost:9000', access_key='minioadmin', secret_key='minioadmin', secure=False)\n        if not client.bucket_exists('neuralake-bucket'):\n            client.make_bucket('neuralake-bucket')\n        print('âœ… MinIO bucket created')\n        \"\n    \n    - name: Run production verification tests\n      env:\n        NEURALAKE_ENV: LOCAL\n      run: poetry run python scripts/production_verification.py\n\n  security-scan:\n    name: Security Scanning\n    runs-on: ubuntu-latest\n    defaults:\n      run:\n        working-directory: ./neuralake\n    \n    steps:\n    - name: Checkout code\n      uses: actions/checkout@v4\n    \n    - name: Set up Python\n      uses: actions/setup-python@v4\n      with:\n        python-version: ${{ env.PYTHON_VERSION }}\n    \n    - name: Install Poetry\n      uses: snok/install-poetry@v1\n      with:\n        version: 1.6.1\n    \n    - name: Install dependencies\n      run: poetry install\n    \n    - name: Run safety check for vulnerabilities\n      run: |\n        poetry add --group dev safety\n        poetry run safety check\n    \n    - name: Run bandit security linting\n      run: |\n        poetry add --group dev bandit\n        poetry run bandit -r src/ scripts/ -f json -o bandit-report.json\n      continue-on-error: true\n    \n    - name: Upload security report\n      uses: actions/upload-artifact@v3\n      if: always()\n      with:\n        name: security-reports\n        path: neuralake/bandit-report.json\n```\n\n**Commands to test locally:**\n```bash\n# Create directory structure\nmkdir -p .github/workflows/\n\n# Validate YAML syntax\npython -c \"import yaml; yaml.safe_load(open('.github/workflows/ci.yml'))\"\n\n# Test workflow locally (if act is installed)\nact -j lint-and-format\n```",
          "status": "done",
          "dependencies": [],
          "parentTaskId": 3
        },
        {
          "id": 5.4,
          "title": "Configure comprehensive testing integration in CI workflow",
          "description": "Add complete test execution, coverage reporting, and production verification to the GitHub Actions CI pipeline",
          "details": "**Files to modify:**\n- `.github/workflows/ci.yml` (extend existing workflow)\n\n**Additional job to add:**\n```yaml\n  test-production-verification:\n    name: Production Verification Tests\n    runs-on: ubuntu-latest\n    needs: lint-and-format\n    defaults:\n      run:\n        working-directory: ./neuralake\n    \n    services:\n      minio:\n        image: minio/minio:latest\n        ports:\n          - 9000:9000\n          - 9001:9001\n        env:\n          MINIO_ROOT_USER: minioadmin\n          MINIO_ROOT_PASSWORD: minioadmin\n        options: >-\n          --health-cmd \"curl -f http://localhost:9000/minio/health/live\"\n          --health-interval 30s\n          --health-timeout 20s\n          --health-retries 5\n        volumes:\n          - /tmp/minio-data:/data\n    \n    steps:\n    - name: Checkout code\n      uses: actions/checkout@v4\n    \n    - name: Set up Python\n      uses: actions/setup-python@v4\n      with:\n        python-version: ${{ env.PYTHON_VERSION }}\n    \n    - name: Install Poetry\n      uses: snok/install-poetry@v1\n    \n    - name: Install dependencies\n      run: poetry install\n    \n    - name: Set up MinIO bucket\n      run: |\n        # Wait for MinIO to be ready\n        sleep 10\n        # Install MinIO client\n        wget https://dl.min.io/client/mc/release/linux-amd64/mc\n        chmod +x mc\n        # Configure MinIO client\n        ./mc alias set local http://localhost:9000 minioadmin minioadmin\n        # Create bucket\n        ./mc mb local/neuralake-bucket\n        # Verify bucket exists\n        ./mc ls local/\n    \n    - name: Run production verification suite\n      env:\n        NEURALAKE_ENV: LOCAL\n        AWS_ACCESS_KEY_ID: minioadmin\n        AWS_SECRET_ACCESS_KEY: minioadmin\n        AWS_ENDPOINT_URL: http://localhost:9000\n      run: |\n        poetry run python scripts/production_verification.py\n    \n    - name: Run unit tests with coverage\n      env:\n        NEURALAKE_ENV: LOCAL\n      run: |\n        poetry run pytest -v --cov=src --cov-report=xml --cov-report=term-missing\n    \n    - name: Upload coverage reports\n      uses: codecov/codecov-action@v3\n      with:\n        file: ./neuralake/coverage.xml\n        flags: unittests\n        name: codecov-umbrella\n```\n\n**Commands to test locally:**\n```bash\ncd neuralake/\n# Start MinIO locally\ndocker-compose up -d minio\n# Run the same tests as CI\nNEURALAKE_ENV=LOCAL poetry run python scripts/production_verification.py\npoetry run pytest -v --cov=src --cov-report=term-missing\n```",
          "status": "done",
          "dependencies": [],
          "parentTaskId": 3
        },
        {
          "id": 6.4,
          "title": "Add status badges and branch protection rules",
          "description": "Configure GitHub repository with CI status badges, branch protection, and automated deployment triggers",
          "details": "**Files to create/modify:**\n\n1. **Update `README.md` with status badges:**\n```markdown\n# Neuralake Data Engineering Platform\n\n[![CI Pipeline](https://github.com/USERNAME/REPO/workflows/CI%20Pipeline/badge.svg)](https://github.com/USERNAME/REPO/actions)\n[![codecov](https://codecov.io/gh/USERNAME/REPO/branch/main/graph/badge.svg)](https://codecov.io/gh/USERNAME/REPO)\n[![Code style: ruff](https://img.shields.io/endpoint?url=https://raw.githubusercontent.com/astral-sh/ruff/main/assets/badge/v2.json)](https://github.com/astral-sh/ruff)\n\n...rest of README...\n```\n\n2. **Create branch protection configuration file `.github/branch-protection.json`:**\n```json\n{\n  \"protection\": {\n    \"required_status_checks\": {\n      \"strict\": true,\n      \"contexts\": [\n        \"lint-and-format\",\n        \"test-production-verification\"\n      ]\n    },\n    \"enforce_admins\": false,\n    \"required_pull_request_reviews\": {\n      \"required_approving_review_count\": 1,\n      \"dismiss_stale_reviews\": true,\n      \"require_code_owner_reviews\": false\n    },\n    \"restrictions\": null\n  }\n}\n```\n\n3. **Create `.github/CODEOWNERS` file:**\n```\n# Global ownership\n* @your-username\n\n# Neuralake core source\n/neuralake/src/ @your-username\n\n# Documentation\n/docs/ @your-username\n\n# CI/CD configurations\n/.github/ @your-username\n```\n\n**GitHub CLI commands to set up branch protection:**\n```bash\n# Install GitHub CLI if not present\n# brew install gh  # macOS\n# Or download from https://cli.github.com/\n\n# Authenticate\ngh auth login\n\n# Set up branch protection (run from repo root)\ngh api repos/:owner/:repo/branches/main/protection \\\n  --method PUT \\\n  --field required_status_checks='{\"strict\":true,\"contexts\":[\"lint-and-format\",\"test-production-verification\"]}' \\\n  --field enforce_admins=false \\\n  --field required_pull_request_reviews='{\"required_approving_review_count\":1,\"dismiss_stale_reviews\":true}' \\\n  --field restrictions=null\n\n# Verify protection is set\ngh api repos/:owner/:repo/branches/main/protection\n```\n\n**Manual GitHub settings (via web interface):**\n1. Go to Settings â†’ Branches\n2. Add rule for `main` branch\n3. Check \"Require status checks to pass\"\n4. Select: lint-and-format, test-production-verification\n5. Check \"Require pull request reviews\"\n6. Set to 1 required reviewer\n7. Save protection rule",
          "status": "done",
          "dependencies": [],
          "parentTaskId": 3
        }
      ]
    },
    {
      "id": 4,
      "title": "Implement Core Delta Lake Table Functionality",
      "description": "Implement a DeltaTable class that leverages the delta-rs library to provide ACID transactions, schema evolution, and time travel capabilities against MinIO storage.",
      "status": "done",
      "dependencies": [
        1
      ],
      "priority": "high",
      "details": "1. Integrate the delta-rs library into our project\n2. Create a DeltaTable struct that wraps delta-rs functionality\n3. Configure delta-rs to work with our MinIO storage backend\n4. Expose ACID transaction capabilities\n5. Support schema evolution through the delta-rs API\n6. Implement time travel queries with timestamp and version\n7. Handle metadata operations for Delta tables",
      "testStrategy": "1. Unit tests for Delta table operations using delta-rs\n2. Integration tests for ACID transaction guarantees against MinIO\n3. Concurrency tests with multiple writers to verify transaction isolation\n4. Schema evolution tests to ensure compatibility\n5. Time travel functionality tests with different versions and timestamps\n6. Test recovery from interrupted transactions\n7. Performance tests for read/write operations with MinIO\n8. Verify compatibility with tables created by the official Delta Lake format",
      "subtasks": [
        {
          "id": 1,
          "title": "Add delta-rs dependencies and configure Poetry",
          "description": "Add delta-rs Python bindings and related dependencies to pyproject.toml and configure them for MinIO integration",
          "details": "**Files to modify:**\n- `neuralake/pyproject.toml`\n\n**Dependencies to add:**\n```toml\n[tool.poetry.dependencies]\ndeltalake = \"^0.15.0\"  # Python bindings for delta-rs\npyarrow = \"^14.0.0\"    # Required for Delta Lake operations\npolars = \"^0.20.0\"     # For DataFrame operations (upgrade if needed)\n\n[tool.poetry.group.dev.dependencies]\nmoto = \"^4.2.0\"        # For mocking AWS services\npytest-asyncio = \"^0.21.0\"  # For async testing\n```\n\n**Installation steps:**\n```bash\ncd neuralake/\npoetry add deltalake pyarrow\npoetry add --group dev moto pytest-asyncio\npoetry install\n```\n\n**Verify installation:**\n```bash\npoetry run python -c \"\nimport deltalake\nimport pyarrow\nprint(f'âœ… deltalake: {deltalake.__version__}')\nprint(f'âœ… pyarrow: {pyarrow.__version__}')\n\"\n```\n\n**Configuration file to create - `neuralake/src/delta_config.py`:**\n```python\n\"\"\"\nDelta Lake configuration for MinIO integration.\n\"\"\"\nimport os\nfrom typing import Dict, Any\n\ndef get_delta_storage_options() -> Dict[str, Any]:\n    \"\"\"Get storage options for Delta Lake with MinIO.\"\"\"\n    return {\n        \"AWS_ENDPOINT_URL\": os.getenv(\"AWS_ENDPOINT_URL\", \"http://localhost:9000\"),\n        \"AWS_ACCESS_KEY_ID\": os.getenv(\"AWS_ACCESS_KEY_ID\", \"minioadmin\"),\n        \"AWS_SECRET_ACCESS_KEY\": os.getenv(\"AWS_SECRET_ACCESS_KEY\", \"minioadmin\"),\n        \"AWS_REGION\": os.getenv(\"AWS_REGION\", \"us-east-1\"),\n        \"AWS_S3_ALLOW_UNSAFE_RENAME\": \"true\",  # Required for MinIO\n        \"AWS_ALLOW_HTTP\": \"true\",              # Allow HTTP for local MinIO\n    }\n\ndef get_delta_table_uri(table_name: str, bucket: str = \"neuralake-bucket\") -> str:\n    \"\"\"Get S3 URI for Delta table.\"\"\"\n    return f\"s3://{bucket}/delta-tables/{table_name}\"\n```\n\n**Error handling:**\n- If deltalake installation fails, check Rust compiler is available\n- For M1 Macs, may need: `export ARCHFLAGS=\"-arch arm64\"`\n- If pyarrow conflicts occur, check version compatibility matrix",
          "status": "done",
          "dependencies": [],
          "parentTaskId": 4
        },
        {
          "id": 2,
          "title": "Implement core DeltaTable class with ACID operations",
          "description": "Create a comprehensive DeltaTable wrapper class that provides high-level interface for Delta Lake operations with proper error handling and logging",
          "details": "**File to create - `neuralake/src/delta_table.py`:**\n```python\n\"\"\"\nDeltaTable class providing high-level interface for Delta Lake operations.\n\"\"\"\nimport logging\nfrom typing import Dict, Any, Optional, List, Union\nfrom pathlib import Path\nimport polars as pl\nfrom deltalake import DeltaTable as DeltaTableCore, write_deltalake\nfrom deltalake.exceptions import DeltaError\nfrom .delta_config import get_delta_storage_options, get_delta_table_uri\n\nlogger = logging.getLogger(__name__)\n\nclass DeltaTable:\n    \\\"\\\"\\\"High-level wrapper for Delta Lake operations with neuralake integration.\\\"\\\"\\\"\n    \n    def __init__(self, table_name: str, bucket: str = \"neuralake-bucket\"):\n        \\\"\\\"\\\"Initialize DeltaTable.\n        \n        Args:\n            table_name: Name of the Delta table\n            bucket: S3 bucket name\n        \\\"\\\"\\\"\n        self.table_name = table_name\n        self.bucket = bucket\n        self.table_uri = get_delta_table_uri(table_name, bucket)\n        self.storage_options = get_delta_storage_options()\n        self._delta_table: Optional[DeltaTableCore] = None\n        \n        logger.info(f\"Initialized DeltaTable: {self.table_uri}\")\n    \n    def _load_table(self) -> DeltaTableCore:\n        \\\"\\\"\\\"Load or reload the Delta table.\\\"\\\"\\\"\n        try:\n            self._delta_table = DeltaTableCore(\n                self.table_uri, \n                storage_options=self.storage_options\n            )\n            return self._delta_table\n        except DeltaError as e:\n            logger.error(f\"Failed to load Delta table {self.table_uri}: {e}\")\n            raise\n    \n    def exists(self) -> bool:\n        \\\"\\\"\\\"Check if Delta table exists.\\\"\\\"\\\"\n        try:\n            self._load_table()\n            return True\n        except DeltaError:\n            return False\n    \n    def create(self, df: pl.DataFrame, schema: Optional[Dict[str, str]] = None, \n               partition_by: Optional[List[str]] = None) -> None:\n        \\\"\\\"\\\"Create a new Delta table.\n        \n        Args:\n            df: Polars DataFrame with initial data\n            schema: Optional schema definition\n            partition_by: Optional list of columns to partition by\n        \\\"\\\"\\\"\n        try:\n            # Convert Polars to PyArrow for Delta Lake\n            arrow_table = df.to_arrow()\n            \n            logger.info(f\"Creating Delta table {self.table_uri} with {len(df)} rows\")\n            \n            write_deltalake(\n                table_or_uri=self.table_uri,\n                data=arrow_table,\n                mode=\"error\",  # Fail if table exists\n                storage_options=self.storage_options,\n                partition_by=partition_by,\n                schema_mode=\"merge\" if schema else None\n            )\n            \n            logger.info(f\"âœ… Created Delta table {self.table_uri}\")\n            \n        except DeltaError as e:\n            logger.error(f\"Failed to create Delta table: {e}\")\n            raise\n    \n    def append(self, df: pl.DataFrame) -> None:\n        \\\"\\\"\\\"Append data to existing Delta table.\n        \n        Args:\n            df: Polars DataFrame to append\n        \\\"\\\"\\\"\n        try:\n            arrow_table = df.to_arrow()\n            \n            logger.info(f\"Appending {len(df)} rows to {self.table_uri}\")\n            \n            write_deltalake(\n                table_or_uri=self.table_uri,\n                data=arrow_table,\n                mode=\"append\",\n                storage_options=self.storage_options,\n                schema_mode=\"merge\"\n            )\n            \n            logger.info(f\"âœ… Appended {len(df)} rows to {self.table_uri}\")\n            \n        except DeltaError as e:\n            logger.error(f\"Failed to append to Delta table: {e}\")\n            raise\n    \n    def overwrite(self, df: pl.DataFrame) -> None:\n        \\\"\\\"\\\"Overwrite Delta table with new data.\n        \n        Args:\n            df: Polars DataFrame to write\n        \\\"\\\"\\\"\n        try:\n            arrow_table = df.to_arrow()\n            \n            logger.info(f\"Overwriting {self.table_uri} with {len(df)} rows\")\n            \n            write_deltalake(\n                table_or_uri=self.table_uri,\n                data=arrow_table,\n                mode=\"overwrite\",\n                storage_options=self.storage_options,\n                schema_mode=\"overwrite\"\n            )\n            \n            logger.info(f\"âœ… Overwrote {self.table_uri} with {len(df)} rows\")\n            \n        except DeltaError as e:\n            logger.error(f\"Failed to overwrite Delta table: {e}\")\n            raise\n    \n    def read(self, version: Optional[int] = None) -> pl.DataFrame:\n        \\\"\\\"\\\"Read data from Delta table.\n        \n        Args:\n            version: Optional version for time travel\n            \n        Returns:\n            Polars DataFrame\n        \\\"\\\"\\\"\n        try:\n            delta_table = self._load_table()\n            \n            if version is not None:\n                logger.info(f\"Reading {self.table_uri} at version {version}\")\n                delta_table.load_version(version)\n            else:\n                logger.info(f\"Reading latest version of {self.table_uri}\")\n            \n            # Read as PyArrow then convert to Polars\n            arrow_table = delta_table.to_pyarrow_table()\n            df = pl.from_arrow(arrow_table)\n            \n            logger.info(f\"âœ… Read {len(df)} rows from {self.table_uri}\")\n            return df\n            \n        except DeltaError as e:\n            logger.error(f\"Failed to read Delta table: {e}\")\n            raise\n    \n    def vacuum(self, retention_hours: int = 168) -> None:\n        \\\"\\\"\\\"Run vacuum to remove old files.\n        \n        Args:\n            retention_hours: Hours to retain old files (default: 7 days)\n        \\\"\\\"\\\"\n        try:\n            delta_table = self._load_table()\n            \n            logger.info(f\"Running vacuum on {self.table_uri} (retention: {retention_hours}h)\")\n            \n            delta_table.vacuum(retention_hours * 3600)  # Convert to seconds\n            \n            logger.info(f\"âœ… Vacuum completed for {self.table_uri}\")\n            \n        except DeltaError as e:\n            logger.error(f\"Failed to vacuum Delta table: {e}\")\n            raise\n    \n    def get_schema(self) -> Dict[str, str]:\n        \\\"\\\"\\\"Get table schema.\n        \n        Returns:\n            Dictionary mapping column names to types\n        \\\"\\\"\\\"\n        try:\n            delta_table = self._load_table()\n            schema = delta_table.schema().to_pyarrow()\n            \n            return {field.name: str(field.type) for field in schema}\n            \n        except DeltaError as e:\n            logger.error(f\"Failed to get schema for {self.table_uri}: {e}\")\n            raise\n    \n    def get_history(self, limit: int = 20) -> pl.DataFrame:\n        \\\"\\\"\\\"Get table history.\n        \n        Args:\n            limit: Maximum number of history entries\n            \n        Returns:\n            Polars DataFrame with history\n        \\\"\\\"\\\"\n        try:\n            delta_table = self._load_table()\n            history = delta_table.history(limit)\n            \n            return pl.from_arrow(history)\n            \n        except DeltaError as e:\n            logger.error(f\"Failed to get history for {self.table_uri}: {e}\")\n            raise\n    \n    def optimize(self) -> None:\n        \\\"\\\"\\\"Optimize table by compacting small files.\\\"\\\"\\\"\n        try:\n            delta_table = self._load_table()\n            \n            logger.info(f\"Optimizing {self.table_uri}\")\n            \n            delta_table.optimize.compact()\n            \n            logger.info(f\"âœ… Optimization completed for {self.table_uri}\")\n            \n        except DeltaError as e:\n            logger.error(f\"Failed to optimize Delta table: {e}\")\n            raise\n```\n\n**Test file to create - `neuralake/tests/test_delta_table.py`:**\n```python\n\"\"\"\nTests for DeltaTable class.\n\"\"\"\nimport pytest\nimport polars as pl\nfrom src.delta_table import DeltaTable\nfrom src.delta_config import get_delta_storage_options\n\n@pytest.fixture\ndef sample_df():\n    \\\"\\\"\\\"Sample DataFrame for testing.\\\"\\\"\\\"\n    return pl.DataFrame({\n        \"id\": [1, 2, 3],\n        \"name\": [\"Alice\", \"Bob\", \"Charlie\"],\n        \"value\": [10.5, 20.3, 30.1]\n    })\n\n@pytest.fixture\ndef test_table():\n    \\\"\\\"\\\"Test DeltaTable instance.\\\"\\\"\\\"\n    return DeltaTable(\"test_table_pytest\")\n\ndef test_delta_table_creation(test_table, sample_df):\n    \\\"\\\"\\\"Test creating a new Delta table.\\\"\\\"\\\"\n    # Clean up if exists\n    if test_table.exists():\n        test_table.overwrite(pl.DataFrame())\n    \n    test_table.create(sample_df)\n    assert test_table.exists()\n    \n    # Read back and verify\n    result_df = test_table.read()\n    assert len(result_df) == 3\n    assert result_df.columns == [\"id\", \"name\", \"value\"]\n\ndef test_delta_table_append(test_table, sample_df):\n    \\\"\\\"\\\"Test appending to Delta table.\\\"\\\"\\\"\n    if not test_table.exists():\n        test_table.create(sample_df)\n    \n    # Append more data\n    new_df = pl.DataFrame({\n        \"id\": [4, 5],\n        \"name\": [\"David\", \"Eve\"],\n        \"value\": [40.4, 50.5]\n    })\n    \n    test_table.append(new_df)\n    \n    result_df = test_table.read()\n    assert len(result_df) >= 2  # At least the appended data\n```\n\n**Commands to test:**\n```bash\ncd neuralake/\npoetry run python -c \"from src.delta_table import DeltaTable; print('âœ… DeltaTable imported successfully')\"\npoetry run pytest tests/test_delta_table.py -v\n```",
          "status": "done",
          "dependencies": [],
          "parentTaskId": 4
        },
        {
          "id": 3,
          "title": "Implement schema evolution and time travel functionality",
          "description": "Add comprehensive support for schema evolution and time travel operations with detailed examples and error handling",
          "details": "**File to create - `neuralake/src/delta_operations.py`:**\n```python\n\"\"\"\nAdvanced Delta Lake operations: schema evolution and time travel.\n\"\"\"\nimport logging\nfrom typing import Dict, Any, Optional, List, Union\nfrom datetime import datetime, timedelta\nimport polars as pl\nfrom deltalake import DeltaTable as DeltaTableCore\nfrom deltalake.exceptions import DeltaError\nfrom .delta_table import DeltaTable\n\nlogger = logging.getLogger(__name__)\n\nclass DeltaTimeTravel:\n    \\\"\\\"\\\"Time travel operations for Delta tables.\\\"\\\"\\\"\n    \n    def __init__(self, delta_table: DeltaTable):\n        self.delta_table = delta_table\n        self.table_uri = delta_table.table_uri\n        self.storage_options = delta_table.storage_options\n    \n    def read_at_timestamp(self, timestamp: Union[str, datetime]) -> pl.DataFrame:\n        \\\"\\\"\\\"Read table as it existed at a specific timestamp.\n        \n        Args:\n            timestamp: ISO timestamp string or datetime object\n            \n        Returns:\n            Polars DataFrame\n        \\\"\\\"\\\"\n        try:\n            if isinstance(timestamp, datetime):\n                timestamp_str = timestamp.isoformat()\n            else:\n                timestamp_str = timestamp\n            \n            logger.info(f\"Reading {self.table_uri} at timestamp {timestamp_str}\")\n            \n            # Create Delta table instance with timestamp\n            delta_table = DeltaTableCore(\n                self.table_uri,\n                storage_options=self.storage_options\n            )\n            \n            # Load specific timestamp\n            delta_table.load_as_version(timestamp_str)\n            \n            # Convert to Polars\n            arrow_table = delta_table.to_pyarrow_table()\n            df = pl.from_arrow(arrow_table)\n            \n            logger.info(f\"âœ… Read {len(df)} rows from {self.table_uri} at {timestamp_str}\")\n            return df\n            \n        except DeltaError as e:\n            logger.error(f\"Failed to read at timestamp {timestamp_str}: {e}\")\n            raise\n    \n    def read_at_version(self, version: int) -> pl.DataFrame:\n        \\\"\\\"\\\"Read table at a specific version number.\n        \n        Args:\n            version: Version number to read\n            \n        Returns:\n            Polars DataFrame\n        \\\"\\\"\\\"\n        return self.delta_table.read(version=version)\n    \n    def compare_versions(self, version1: int, version2: int) -> Dict[str, Any]:\n        \\\"\\\"\\\"Compare two versions of the table.\n        \n        Args:\n            version1: First version to compare\n            version2: Second version to compare\n            \n        Returns:\n            Dictionary with comparison results\n        \\\"\\\"\\\"\n        try:\n            df1 = self.read_at_version(version1)\n            df2 = self.read_at_version(version2)\n            \n            comparison = {\n                \"version1\": {\n                    \"version\": version1,\n                    \"rows\": len(df1),\n                    \"columns\": df1.columns,\n                    \"schema\": {col: str(dtype) for col, dtype in zip(df1.columns, df1.dtypes)}\n                },\n                \"version2\": {\n                    \"version\": version2,\n                    \"rows\": len(df2),\n                    \"columns\": df2.columns,\n                    \"schema\": {col: str(dtype) for col, dtype in zip(df2.columns, df2.dtypes)}\n                },\n                \"differences\": {\n                    \"row_count_diff\": len(df2) - len(df1),\n                    \"new_columns\": set(df2.columns) - set(df1.columns),\n                    \"removed_columns\": set(df1.columns) - set(df2.columns),\n                    \"schema_changes\": []\n                }\n            }\n            \n            # Check for schema changes\n            for col in set(df1.columns) & set(df2.columns):\n                dtype1 = str(dict(zip(df1.columns, df1.dtypes))[col])\n                dtype2 = str(dict(zip(df2.columns, df2.dtypes))[col])\n                if dtype1 != dtype2:\n                    comparison[\"differences\"][\"schema_changes\"].append({\n                        \"column\": col,\n                        \"old_type\": dtype1,\n                        \"new_type\": dtype2\n                    })\n            \n            return comparison\n            \n        except Exception as e:\n            logger.error(f\"Failed to compare versions {version1} and {version2}: {e}\")\n            raise\n\nclass DeltaSchemaEvolution:\n    \\\"\\\"\\\"Schema evolution operations for Delta tables.\\\"\\\"\\\"\n    \n    def __init__(self, delta_table: DeltaTable):\n        self.delta_table = delta_table\n    \n    def add_column(self, df_with_new_column: pl.DataFrame, \n                  merge_schema: bool = True) -> None:\n        \\\"\\\"\\\"Add new column(s) to the table by appending data with additional columns.\n        \n        Args:\n            df_with_new_column: DataFrame with new column(s)\n            merge_schema: Whether to merge schemas automatically\n        \\\"\\\"\\\"\n        try:\n            current_schema = self.delta_table.get_schema()\n            new_columns = set(df_with_new_column.columns) - set(current_schema.keys())\n            \n            logger.info(f\"Adding columns to {self.delta_table.table_uri}: {new_columns}\")\n            \n            if merge_schema:\n                # Use append with schema merging\n                self.delta_table.append(df_with_new_column)\n            else:\n                # Manual schema validation\n                for col in new_columns:\n                    logger.info(f\"New column: {col} ({df_with_new_column[col].dtype})\")\n                \n                self.delta_table.append(df_with_new_column)\n            \n            logger.info(f\"âœ… Successfully added columns: {new_columns}\")\n            \n        except Exception as e:\n            logger.error(f\"Failed to add columns: {e}\")\n            raise\n    \n    def get_schema_history(self) -> pl.DataFrame:\n        \\\"\\\"\\\"Get the schema evolution history.\n        \n        Returns:\n            DataFrame with schema changes over time\n        \\\"\\\"\\\"\n        try:\n            history = self.delta_table.get_history()\n            \n            # Extract schema information from each version\n            schema_history = []\n            \n            for row in history.iter_rows(named=True):\n                version = row.get('version', 0)\n                timestamp = row.get('timestamp')\n                \n                try:\n                    # Read schema at this version\n                    time_travel = DeltaTimeTravel(self.delta_table)\n                    df_at_version = time_travel.read_at_version(version)\n                    \n                    schema_info = {\n                        'version': version,\n                        'timestamp': timestamp,\n                        'column_count': len(df_at_version.columns),\n                        'columns': df_at_version.columns,\n                        'schema': {col: str(dtype) for col, dtype in \n                                 zip(df_at_version.columns, df_at_version.dtypes)}\n                    }\n                    schema_history.append(schema_info)\n                    \n                except Exception as e:\n                    logger.warning(f\"Could not read schema for version {version}: {e}\")\n            \n            return pl.DataFrame(schema_history)\n            \n        except Exception as e:\n            logger.error(f\"Failed to get schema history: {e}\")\n            raise\n\n# Convenience functions\ndef time_travel_query(table_name: str, timestamp: str, \n                     bucket: str = \"neuralake-bucket\") -> pl.DataFrame:\n    \\\"\\\"\\\"Convenience function for time travel queries.\n    \n    Args:\n        table_name: Name of the Delta table\n        timestamp: ISO timestamp string\n        bucket: S3 bucket name\n        \n    Returns:\n        Polars DataFrame\n    \\\"\\\"\\\"\n    delta_table = DeltaTable(table_name, bucket)\n    time_travel = DeltaTimeTravel(delta_table)\n    return time_travel.read_at_timestamp(timestamp)\n\ndef evolve_schema_example(table_name: str, bucket: str = \"neuralake-bucket\") -> None:\n    \\\"\\\"\\\"Example of schema evolution workflow.\n    \n    Args:\n        table_name: Name of the Delta table\n        bucket: S3 bucket name\n    \\\"\\\"\\\"\n    logger.info(f\"Running schema evolution example for {table_name}\")\n    \n    # Create Delta table\n    delta_table = DeltaTable(table_name, bucket)\n    \n    # Initial data\n    initial_df = pl.DataFrame({\n        \"id\": [1, 2, 3],\n        \"name\": [\"Alice\", \"Bob\", \"Charlie\"],\n        \"value\": [10.5, 20.3, 30.1]\n    })\n    \n    if not delta_table.exists():\n        delta_table.create(initial_df)\n        logger.info(\"Created initial table\")\n    \n    # Add new column\n    evolved_df = pl.DataFrame({\n        \"id\": [4, 5],\n        \"name\": [\"David\", \"Eve\"],\n        \"value\": [40.4, 50.5],\n        \"category\": [\"A\", \"B\"]  # New column\n    })\n    \n    schema_evolution = DeltaSchemaEvolution(delta_table)\n    schema_evolution.add_column(evolved_df)\n    \n    logger.info(\"Schema evolution completed\")\n    \n    # Demonstrate time travel\n    time_travel = DeltaTimeTravel(delta_table)\n    comparison = time_travel.compare_versions(0, 1)\n    \n    logger.info(f\"Schema changes: {comparison['differences']}\")\n```\n\n**Example script - `neuralake/scripts/delta_examples.py`:**\n```python\n#!/usr/bin/env python3\n\"\"\"\nExamples demonstrating Delta Lake schema evolution and time travel.\n\"\"\"\nimport sys\nfrom pathlib import Path\nsys.path.append(str(Path(__file__).parent.parent / \"src\"))\n\nfrom delta_operations import evolve_schema_example, time_travel_query\nfrom delta_table import DeltaTable\nimport polars as pl\nfrom datetime import datetime, timedelta\n\ndef main():\n    print(\"ðŸ”„ Delta Lake Examples: Schema Evolution & Time Travel\")\n    \n    # 1. Schema Evolution Example\n    print(\"\\n1. Schema Evolution Example:\")\n    evolve_schema_example(\"example_evolution\")\n    \n    # 2. Time Travel Example\n    print(\"\\n2. Time Travel Example:\")\n    table_name = \"example_time_travel\"\n    delta_table = DeltaTable(table_name)\n    \n    # Create initial data\n    df1 = pl.DataFrame({\n        \"timestamp\": [datetime.now()],\n        \"event\": [\"user_login\"],\n        \"user_id\": [123]\n    })\n    \n    if not delta_table.exists():\n        delta_table.create(df1)\n        print(f\"Created table {table_name}\")\n    \n    # Add more data after a delay\n    import time\n    time.sleep(2)\n    \n    df2 = pl.DataFrame({\n        \"timestamp\": [datetime.now()],\n        \"event\": [\"user_logout\"],\n        \"user_id\": [123]\n    })\n    \n    delta_table.append(df2)\n    print(\"Appended new data\")\n    \n    # Time travel queries\n    print(\"\\nTime travel queries:\")\n    latest = delta_table.read()\n    print(f\"Latest version has {len(latest)} rows\")\n    \n    version_0 = delta_table.read(version=0)\n    print(f\"Version 0 has {len(version_0)} rows\")\n    \n    # Schema history\n    from delta_operations import DeltaSchemaEvolution\n    schema_evo = DeltaSchemaEvolution(delta_table)\n    history = schema_evo.get_schema_history()\n    print(f\"\\nSchema history: {len(history)} versions tracked\")\n\nif __name__ == \"__main__\":\n    main()\n```\n\n**Commands to test:**\n```bash\ncd neuralake/\npoetry run python scripts/delta_examples.py\npoetry run python -c \"\nfrom src.delta_operations import DeltaTimeTravel, DeltaSchemaEvolution\nprint('âœ… Advanced Delta operations imported successfully')\n\"\n```",
          "status": "done",
          "dependencies": [],
          "parentTaskId": 4
        },
        {
          "id": 4,
          "title": "Implement comprehensive Delta Lake integration testing",
          "description": "Create comprehensive test suite for Delta Lake operations including ACID transactions, concurrent writes, and performance validation",
          "details": "**File to create - `neuralake/tests/test_delta_integration.py`:**\n```python\n\"\"\"\nComprehensive integration tests for Delta Lake functionality.\n\"\"\"\nimport pytest\nimport polars as pl\nimport tempfile\nimport shutil\nfrom pathlib import Path\nfrom concurrent.futures import ThreadPoolExecutor, as_completed\nfrom unittest.mock import patch\nimport time\nimport os\n\nfrom src.delta_table import DeltaTable\nfrom src.delta_operations import DeltaTimeTravel, DeltaSchemaEvolution\nfrom src.config import get_s3_storage_options, get_delta_table_uri\n\nclass TestDeltaLakeIntegration:\n    \n    @pytest.fixture\n    def temp_table_path(self):\n        \"\"\"Create temporary directory for Delta table testing.\"\"\"\n        temp_dir = tempfile.mkdtemp()\n        yield temp_dir\n        shutil.rmtree(temp_dir)\n    \n    @pytest.fixture\n    def sample_data(self):\n        \"\"\"Generate sample data for testing.\"\"\"\n        return pl.DataFrame({\n            \"id\": range(1, 101),\n            \"name\": [f\"user_{i}\" for i in range(1, 101)],\n            \"value\": [i * 1.5 for i in range(1, 101)],\n            \"category\": [\"A\" if i % 2 == 0 else \"B\" for i in range(1, 101)]\n        })\n    \n    def test_basic_delta_operations(self, temp_table_path, sample_data):\n        \"\"\"Test basic Delta table creation, write, and read operations.\"\"\"\n        table_name = \"test_basic\"\n        \n        # Create Delta table\n        delta_table = DeltaTable(table_name, storage_path=temp_table_path)\n        \n        # Initial write\n        result = delta_table.write_data(sample_data, mode=\"overwrite\")\n        assert result[\"success\"] is True\n        assert result[\"files_added\"] > 0\n        \n        # Read data back\n        read_data = delta_table.read_data()\n        assert read_data.height == 100\n        assert set(read_data.columns) == {\"id\", \"name\", \"value\", \"category\"}\n        \n        # Verify data integrity\n        assert read_data.filter(pl.col(\"id\") == 1)[\"name\"][0] == \"user_1\"\n        assert read_data.filter(pl.col(\"id\") == 50)[\"value\"][0] == 75.0\n    \n    def test_acid_transactions(self, temp_table_path, sample_data):\n        \"\"\"Test ACID transaction properties.\"\"\"\n        table_name = \"test_acid\"\n        delta_table = DeltaTable(table_name, storage_path=temp_table_path)\n        \n        # Initial write\n        delta_table.write_data(sample_data, mode=\"overwrite\")\n        \n        # Test atomicity - simulate failed write\n        corrupt_data = pl.DataFrame({\"wrong_schema\": [1, 2, 3]})\n        \n        with pytest.raises(Exception):\n            delta_table.write_data(corrupt_data, mode=\"append\")\n        \n        # Verify original data is intact (atomicity)\n        read_data = delta_table.read_data()\n        assert read_data.height == 100\n        \n        # Test consistency - concurrent reads during write\n        def concurrent_read():\n            return delta_table.read_data().height\n        \n        # Start a write operation\n        append_data = sample_data.with_columns(pl.col(\"id\") + 100)\n        delta_table.write_data(append_data, mode=\"append\")\n        \n        # Concurrent reads should see consistent state\n        with ThreadPoolExecutor(max_workers=3) as executor:\n            futures = [executor.submit(concurrent_read) for _ in range(5)]\n            results = [future.result() for future in as_completed(futures)]\n        \n        # All reads should return the same count (consistency)\n        assert len(set(results)) <= 2  # Either before or after write\n        assert 100 in results or 200 in results\n    \n    def test_schema_evolution(self, temp_table_path, sample_data):\n        \"\"\"Test schema evolution capabilities.\"\"\"\n        table_name = \"test_schema_evolution\"\n        delta_table = DeltaTable(table_name, storage_path=temp_table_path)\n        \n        # Initial write\n        delta_table.write_data(sample_data, mode=\"overwrite\")\n        \n        # Add new column\n        evolved_data = sample_data.with_columns(\n            pl.lit(\"new_value\").alias(\"new_column\")\n        )\n        \n        result = delta_table.write_data(evolved_data, mode=\"append\", \n                                       schema_mode=\"merge\")\n        assert result[\"success\"] is True\n        \n        # Read evolved data\n        read_data = delta_table.read_data()\n        assert \"new_column\" in read_data.columns\n        assert read_data.height == 200\n        \n        # Verify old records have null for new column\n        old_records = read_data.filter(pl.col(\"id\") <= 100)\n        new_records = read_data.filter(pl.col(\"id\") > 100)\n        \n        assert old_records[\"new_column\"].is_null().sum() == 100\n        assert new_records[\"new_column\"].is_null().sum() == 0\n    \n    def test_time_travel(self, temp_table_path, sample_data):\n        \"\"\"Test time travel functionality.\"\"\"\n        table_name = \"test_time_travel\"\n        delta_table = DeltaTable(table_name, storage_path=temp_table_path)\n        \n        # Version 0: Initial data\n        delta_table.write_data(sample_data, mode=\"overwrite\")\n        \n        # Version 1: Append more data\n        append_data = sample_data.with_columns(pl.col(\"id\") + 100)\n        delta_table.write_data(append_data, mode=\"append\")\n        \n        # Version 2: Update some records\n        update_data = pl.DataFrame({\n            \"id\": [1, 2, 3],\n            \"name\": [\"updated_1\", \"updated_2\", \"updated_3\"],\n            \"value\": [999.0, 888.0, 777.0],\n            \"category\": [\"UPDATED\", \"UPDATED\", \"UPDATED\"]\n        })\n        delta_table.merge_data(update_data, merge_key=\"id\")\n        \n        # Test time travel\n        time_travel = DeltaTimeTravel(delta_table)\n        \n        # Read version 0\n        v0_data = time_travel.read_at_version(0)\n        assert v0_data.height == 100\n        assert v0_data.filter(pl.col(\"id\") == 1)[\"name\"][0] == \"user_1\"\n        \n        # Read version 1\n        v1_data = time_travel.read_at_version(1)\n        assert v1_data.height == 200\n        \n        # Read version 2 (current)\n        v2_data = time_travel.read_at_version(2)\n        assert v2_data.filter(pl.col(\"id\") == 1)[\"name\"][0] == \"updated_1\"\n        \n        # Test timestamp-based time travel\n        history = time_travel.get_history()\n        assert len(history) >= 3\n        \n        # Read at specific timestamp\n        first_commit_time = history[0][\"timestamp\"]\n        timestamp_data = time_travel.read_at_timestamp(first_commit_time)\n        assert timestamp_data.height == 100\n    \n    @pytest.mark.slow\n    def test_performance_benchmarks(self, temp_table_path):\n        \"\"\"Test performance characteristics of Delta operations.\"\"\"\n        table_name = \"test_performance\"\n        delta_table = DeltaTable(table_name, storage_path=temp_table_path)\n        \n        # Generate larger dataset for performance testing\n        large_data = pl.DataFrame({\n            \"id\": range(1, 10001),\n            \"data\": [f\"data_value_{i}\" for i in range(1, 10001)],\n            \"numeric\": [i * 0.1 for i in range(1, 10001)]\n        })\n        \n        # Benchmark write performance\n        start_time = time.time()\n        result = delta_table.write_data(large_data, mode=\"overwrite\")\n        write_time = time.time() - start_time\n        \n        assert result[\"success\"] is True\n        assert write_time < 10.0  # Should complete within 10 seconds\n        \n        # Benchmark read performance\n        start_time = time.time()\n        read_data = delta_table.read_data()\n        read_time = time.time() - start_time\n        \n        assert read_data.height == 10000\n        assert read_time < 5.0  # Should complete within 5 seconds\n        \n        # Benchmark filtered read performance\n        start_time = time.time()\n        filtered_data = delta_table.read_data(\n            filters=[(\"id\", \">\", 5000), (\"id\", \"<\", 7500)]\n        )\n        filter_time = time.time() - start_time\n        \n        assert filtered_data.height == 2499\n        assert filter_time < 3.0  # Should complete within 3 seconds\n        \n        # Log performance metrics\n        print(f\"Performance Benchmarks:\")\n        print(f\"  Write (10k rows): {write_time:.3f}s\")\n        print(f\"  Read (10k rows): {read_time:.3f}s\")\n        print(f\"  Filtered read: {filter_time:.3f}s\")\n    \n    def test_concurrent_operations(self, temp_table_path, sample_data):\n        \"\"\"Test concurrent write operations with DynamoDB locking.\"\"\"\n        table_name = \"test_concurrent\"\n        \n        def write_worker(worker_id):\n            \"\"\"Worker function for concurrent writes.\"\"\"\n            worker_data = sample_data.with_columns(\n                pl.col(\"id\") + (worker_id * 1000),\n                pl.lit(f\"worker_{worker_id}\").alias(\"worker\")\n            )\n            \n            delta_table = DeltaTable(table_name, storage_path=temp_table_path)\n            return delta_table.write_data(worker_data, mode=\"append\")\n        \n        # Execute concurrent writes\n        with ThreadPoolExecutor(max_workers=3) as executor:\n            futures = [executor.submit(write_worker, i) for i in range(3)]\n            results = [future.result() for future in as_completed(futures)]\n        \n        # All writes should succeed\n        assert all(result[\"success\"] for result in results)\n        \n        # Verify final state\n        delta_table = DeltaTable(table_name, storage_path=temp_table_path)\n        final_data = delta_table.read_data()\n        \n        # Should have data from all workers\n        assert final_data.height == 300  # 100 rows Ã— 3 workers\n        \n        # Verify worker data separation\n        worker_counts = final_data.group_by(\"worker\").count()\n        assert worker_counts.height == 3\n        assert all(count == 100 for count in worker_counts[\"count\"])\n\n# Additional test configuration\n@pytest.mark.integration\ndef test_s3_integration():\n    \"\"\"Test Delta Lake operations against MinIO S3.\"\"\"\n    # This test requires running MinIO service\n    storage_options = get_s3_storage_options()\n    \n    if not storage_options.get(\"endpoint_url\"):\n        pytest.skip(\"S3 endpoint not configured\")\n    \n    table_uri = get_delta_table_uri(\"test_s3_integration\")\n    \n    # Test basic S3 operations\n    test_data = pl.DataFrame({\n        \"test_id\": [1, 2, 3],\n        \"test_value\": [\"a\", \"b\", \"c\"]\n    })\n    \n    # Write to S3-backed Delta table\n    delta_table = DeltaTable(\"test_s3_integration\")\n    result = delta_table.write_data(test_data, mode=\"overwrite\")\n    \n    assert result[\"success\"] is True\n    \n    # Read back from S3\n    read_data = delta_table.read_data()\n    assert read_data.height == 3\n    assert set(read_data.columns) == {\"test_id\", \"test_value\"}\n```\n\n**File to create - `neuralake/tests/conftest.py` (pytest configuration):**\n```python\n\"\"\"\nPytest configuration for Delta Lake tests.\n\"\"\"\nimport pytest\nimport os\nfrom src.config import set_environment\n\ndef pytest_configure(config):\n    \"\"\"Configure pytest with custom markers.\"\"\"\n    config.addinivalue_line(\n        \"markers\", \"slow: marks tests as slow (deselect with '-m \\\"not slow\\\"')\"\n    )\n    config.addinivalue_line(\n        \"markers\", \"integration: marks tests as integration tests\"\n    )\n\n@pytest.fixture(scope=\"session\", autouse=True)\ndef setup_test_environment():\n    \"\"\"Set up test environment configuration.\"\"\"\n    os.environ[\"NEURALAKE_ENV\"] = \"LOCAL\"\n    set_environment(\"LOCAL\")\n    yield\n    # Cleanup after tests\n```\n\n**Commands to run tests:**\n```bash\ncd neuralake/\n\n# Run all Delta tests\npoetry run pytest tests/test_delta_integration.py -v\n\n# Run without slow tests\npoetry run pytest tests/test_delta_integration.py -v -m \"not slow\"\n\n# Run with coverage\npoetry run pytest tests/test_delta_integration.py -v --cov=src\n\n# Run integration tests only (requires MinIO)\ndocker-compose up -d minio\npoetry run pytest tests/test_delta_integration.py -v -m integration\n```",
          "status": "done",
          "dependencies": [],
          "parentTaskId": 4
        }
      ]
    },
    {
      "id": 5,
      "title": "Implement Code as a Catalog Core & Static Site Generation",
      "description": "Following the Neuralink model, create the core Python classes that enable defining tables in code using decorators and class definitions, with automated static HTML site generation from table definitions.",
      "status": "done",
      "dependencies": [
        3,
        4
      ],
      "priority": "high",
      "details": "1. Create core Python catalog classes (`Catalog`, `ModuleDatabase`, table definitions) following Neuralink's approach\n2. Implement table definition decorators and class-based definitions for both `ParquetTable` and `DeltaTable`\n3. Support full metadata extraction from code definitions\n4. Implement automated static HTML site generation for visual browsing and discovery\n5. Generate Python client code snippets for users to copy-paste\n6. Ensure the catalog system automatically generates comprehensive documentation\n7. Eliminate the need for manually maintained documentation that can become stale\n8. Support the 'Code as a Catalog' philosophy where the catalog is generated directly from version-controlled code",
      "testStrategy": "1. Unit tests for catalog class functionality\n2. Test table registration and discovery\n3. Verify static site generation with sample catalog\n4. Test HTML template rendering\n5. Validate schema visualization accuracy\n6. Test search functionality\n7. Verify documentation generation from code comments\n8. Test integration with MinIO for table location resolution\n9. Test both ParquetTable and DeltaTable definitions",
      "subtasks": [
        {
          "id": "5.1",
          "title": "Design and implement core catalog classes",
          "status": "done",
          "description": "Create the foundational catalog structures that enable the code-as-a-catalog approach following Neuralink's model"
        },
        {
          "id": "5.2",
          "title": "Implement table definition decorators and classes",
          "status": "done",
          "description": "Create table definition structures with decorators supporting both ParquetTable and DeltaTable with full metadata"
        },
        {
          "id": "5.3",
          "title": "Create static site generator",
          "status": "done",
          "description": "Implement static HTML site generation for catalog documentation and browsing"
        },
        {
          "id": "5.4",
          "title": "Add Python code snippet generation",
          "status": "done",
          "description": "Generate copy-paste Python client code snippets from table definitions"
        },
        {
          "id": 6.4,
          "title": "Implement table decorator system and catalog core",
          "description": "Create the foundational decorator system that allows defining tables in code, following Neuralink's 'Code as a Catalog' philosophy",
          "details": "**File to create - `neuralake/src/catalog_core.py`:**\n```python\n\"\"\"\nCore catalog system implementing 'Code as a Catalog' philosophy.\nAllows defining tables in code using decorators and class definitions.\n\"\"\"\nimport inspect\nimport logging\nfrom typing import Dict, Any, Optional, List, Callable, Union, Type\nfrom dataclasses import dataclass, field\nfrom enum import Enum\nimport polars as pl\nfrom pathlib import Path\nimport json\nfrom datetime import datetime\n\nfrom .delta_table import DeltaTable\nfrom .config import get_config\n\nlogger = logging.getLogger(__name__)\n\nclass TableType(Enum):\n    \"\"\"Types of tables supported in the catalog.\"\"\"\n    PARQUET = \"parquet\"\n    DELTA = \"delta\"\n    FUNCTION = \"function\"\n    VIEW = \"view\"\n\n@dataclass\nclass TableMetadata:\n    \"\"\"Metadata for catalog tables.\"\"\"\n    name: str\n    table_type: TableType\n    description: str = \"\"\n    schema: Optional[Dict[str, str]] = None\n    partition_columns: List[str] = field(default_factory=list)\n    source_module: str = \"\"\n    source_function: str = \"\"\n    created_at: datetime = field(default_factory=datetime.now)\n    tags: List[str] = field(default_factory=list)\n    owner: str = \"\"\n    \n    def to_dict(self) -> Dict[str, Any]:\n        \"\"\"Convert metadata to dictionary for serialization.\"\"\"\n        return {\n            \"name\": self.name,\n            \"table_type\": self.table_type.value,\n            \"description\": self.description,\n            \"schema\": self.schema or {},\n            \"partition_columns\": self.partition_columns,\n            \"source_module\": self.source_module,\n            \"source_function\": self.source_function,\n            \"created_at\": self.created_at.isoformat(),\n            \"tags\": self.tags,\n            \"owner\": self.owner\n        }\n\nclass CatalogRegistry:\n    \"\"\"Central registry for all catalog tables.\"\"\"\n    \n    def __init__(self):\n        self._tables: Dict[str, TableMetadata] = {}\n        self._table_functions: Dict[str, Callable] = {}\n        self._table_objects: Dict[str, Any] = {}\n    \n    def register_table(self, metadata: TableMetadata, \n                      table_obj: Any = None, \n                      table_func: Callable = None):\n        \"\"\"Register a table in the catalog.\"\"\"\n        self._tables[metadata.name] = metadata\n        \n        if table_obj is not None:\n            self._table_objects[metadata.name] = table_obj\n        \n        if table_func is not None:\n            self._table_functions[metadata.name] = table_func\n        \n        logger.info(f\"Registered table '{metadata.name}' of type {metadata.table_type.value}\")\n    \n    def get_table_metadata(self, name: str) -> Optional[TableMetadata]:\n        \"\"\"Get metadata for a table.\"\"\"\n        return self._tables.get(name)\n    \n    def get_table_function(self, name: str) -> Optional[Callable]:\n        \"\"\"Get function for a function-based table.\"\"\"\n        return self._table_functions.get(name)\n    \n    def get_table_object(self, name: str) -> Any:\n        \"\"\"Get table object (ParquetTable, DeltaTable, etc.).\"\"\"\n        return self._table_objects.get(name)\n    \n    def list_tables(self, table_type: Optional[TableType] = None) -> List[str]:\n        \"\"\"List all tables, optionally filtered by type.\"\"\"\n        if table_type is None:\n            return list(self._tables.keys())\n        \n        return [name for name, metadata in self._tables.items() \n                if metadata.table_type == table_type]\n    \n    def export_metadata(self) -> Dict[str, Any]:\n        \"\"\"Export all table metadata for static site generation.\"\"\"\n        return {\n            \"tables\": {name: metadata.to_dict() \n                      for name, metadata in self._tables.items()},\n            \"export_time\": datetime.now().isoformat(),\n            \"total_tables\": len(self._tables)\n        }\n\n# Global registry instance\n_catalog_registry = CatalogRegistry()\n\ndef table(name: Optional[str] = None, \n         description: str = \"\",\n         table_type: TableType = TableType.FUNCTION,\n         schema: Optional[Dict[str, str]] = None,\n         partition_columns: List[str] = None,\n         tags: List[str] = None,\n         owner: str = \"\"):\n    \"\"\"\n    Decorator to register a function as a table in the catalog.\n    \n    Usage:\n        @table(description=\"User data from API\")\n        def users() -> pl.LazyFrame:\n            return pl.LazyFrame({\"id\": [1, 2], \"name\": [\"Alice\", \"Bob\"]})\n    \"\"\"\n    def decorator(func: Callable) -> Callable:\n        table_name = name or func.__name__\n        \n        # Extract docstring if no description provided\n        table_description = description or (func.__doc__ or \"\").strip()\n        \n        # Get module information\n        module_name = func.__module__ if hasattr(func, '__module__') else \"\"\n        \n        # Create metadata\n        metadata = TableMetadata(\n            name=table_name,\n            table_type=table_type,\n            description=table_description,\n            schema=schema,\n            partition_columns=partition_columns or [],\n            source_module=module_name,\n            source_function=func.__name__,\n            tags=tags or [],\n            owner=owner\n        )\n        \n        # Register in global catalog\n        _catalog_registry.register_table(metadata, table_func=func)\n        \n        # Add metadata to function\n        func._catalog_metadata = metadata\n        \n        return func\n    \n    return decorator\n\ndef register_static_table(table_obj: Union[DeltaTable, Any], \n                         name: str,\n                         description: str = \"\",\n                         schema: Optional[Dict[str, str]] = None,\n                         partition_columns: List[str] = None,\n                         tags: List[str] = None,\n                         owner: str = \"\"):\n    \"\"\"\n    Register a static table object (DeltaTable, ParquetTable, etc.) in the catalog.\n    \n    Usage:\n        delta_table = DeltaTable(\"user_events\")\n        register_static_table(delta_table, \"user_events\", \n                             description=\"User interaction events\")\n    \"\"\"\n    # Determine table type\n    if isinstance(table_obj, DeltaTable):\n        table_type = TableType.DELTA\n    else:\n        # Assume ParquetTable or similar\n        table_type = TableType.PARQUET\n    \n    metadata = TableMetadata(\n        name=name,\n        table_type=table_type,\n        description=description,\n        schema=schema,\n        partition_columns=partition_columns or [],\n        tags=tags or [],\n        owner=owner\n    )\n    \n    _catalog_registry.register_table(metadata, table_obj=table_obj)\n\nclass Catalog:\n    \"\"\"\n    High-level catalog interface providing unified access to all tables.\n    \"\"\"\n    \n    def __init__(self, registry: CatalogRegistry = None):\n        self.registry = registry or _catalog_registry\n    \n    def table(self, name: str, **kwargs) -> pl.LazyFrame:\n        \"\"\"\n        Get a table as a Polars LazyFrame.\n        \n        Args:\n            name: Table name\n            **kwargs: Additional arguments passed to table function/object\n        \n        Returns:\n            Polars LazyFrame with the table data\n        \"\"\"\n        metadata = self.registry.get_table_metadata(name)\n        if not metadata:\n            raise ValueError(f\"Table '{name}' not found in catalog\")\n        \n        if metadata.table_type == TableType.FUNCTION:\n            # Execute function to get data\n            func = self.registry.get_table_function(name)\n            if not func:\n                raise ValueError(f\"Function for table '{name}' not found\")\n            \n            result = func(**kwargs)\n            \n            # Ensure we return a LazyFrame\n            if isinstance(result, pl.DataFrame):\n                return result.lazy()\n            elif isinstance(result, pl.LazyFrame):\n                return result\n            else:\n                raise ValueError(f\"Table function '{name}' must return DataFrame or LazyFrame\")\n        \n        elif metadata.table_type in [TableType.DELTA, TableType.PARQUET]:\n            # Get data from table object\n            table_obj = self.registry.get_table_object(name)\n            if not table_obj:\n                raise ValueError(f\"Table object for '{name}' not found\")\n            \n            if isinstance(table_obj, DeltaTable):\n                return table_obj.read_data(**kwargs).lazy()\n            else:\n                # Assume ParquetTable or similar with __call__ method\n                return table_obj(**kwargs).lazy()\n        \n        else:\n            raise ValueError(f\"Unsupported table type: {metadata.table_type}\")\n    \n    def list_tables(self, table_type: Optional[TableType] = None) -> List[str]:\n        \"\"\"List all available tables.\"\"\"\n        return self.registry.list_tables(table_type)\n    \n    def describe_table(self, name: str) -> Dict[str, Any]:\n        \"\"\"Get detailed information about a table.\"\"\"\n        metadata = self.registry.get_table_metadata(name)\n        if not metadata:\n            raise ValueError(f\"Table '{name}' not found in catalog\")\n        \n        # Get sample data to infer schema if not provided\n        try:\n            sample_data = self.table(name).limit(1).collect()\n            inferred_schema = {col: str(dtype) for col, dtype in \n                             zip(sample_data.columns, sample_data.dtypes)}\n        except Exception:\n            inferred_schema = {}\n        \n        return {\n            \"metadata\": metadata.to_dict(),\n            \"inferred_schema\": inferred_schema,\n            \"column_count\": len(inferred_schema),\n            \"available\": True\n        }\n    \n    def export_catalog_metadata(self, output_path: Optional[Path] = None) -> Dict[str, Any]:\n        \"\"\"Export catalog metadata for static site generation.\"\"\"\n        metadata = self.registry.export_metadata()\n        \n        if output_path:\n            output_path.parent.mkdir(parents=True, exist_ok=True)\n            with open(output_path, 'w') as f:\n                json.dump(metadata, f, indent=2, default=str)\n        \n        return metadata\n\n# Global catalog instance\ndefault_catalog = Catalog()\n\n# Convenience functions\ndef get_table(name: str, **kwargs) -> pl.LazyFrame:\n    \"\"\"Get a table from the default catalog.\"\"\"\n    return default_catalog.table(name, **kwargs)\n\ndef list_catalog_tables(table_type: Optional[TableType] = None) -> List[str]:\n    \"\"\"List tables in the default catalog.\"\"\"\n    return default_catalog.list_tables(table_type)\n\ndef describe_catalog_table(name: str) -> Dict[str, Any]:\n    \"\"\"Describe a table in the default catalog.\"\"\"\n    return default_catalog.describe_table(name)\n```\n\n**File to create - `neuralake/tests/test_catalog_core.py`:**\n```python\n\"\"\"\nTest the catalog core functionality.\n\"\"\"\nimport pytest\nimport polars as pl\nfrom src.catalog_core import (\n    table, register_static_table, Catalog, TableType, \n    default_catalog, _catalog_registry\n)\nfrom src.delta_table import DeltaTable\n\n@pytest.fixture(autouse=True)\ndef clear_registry():\n    \"\"\"Clear the registry before each test.\"\"\"\n    _catalog_registry._tables.clear()\n    _catalog_registry._table_functions.clear()\n    _catalog_registry._table_objects.clear()\n    yield\n    # Clear after test too\n    _catalog_registry._tables.clear()\n    _catalog_registry._table_functions.clear()\n    _catalog_registry._table_objects.clear()\n\ndef test_function_table_registration():\n    \"\"\"Test registering function-based tables.\"\"\"\n    \n    @table(description=\"Test user data\")\n    def test_users():\n        \"\"\"User data for testing.\"\"\"\n        return pl.LazyFrame({\n            \"id\": [1, 2, 3],\n            \"name\": [\"Alice\", \"Bob\", \"Charlie\"],\n            \"age\": [25, 30, 35]\n        })\n    \n    # Verify registration\n    tables = default_catalog.list_tables()\n    assert \"test_users\" in tables\n    \n    # Test table access\n    data = default_catalog.table(\"test_users\").collect()\n    assert data.height == 3\n    assert set(data.columns) == {\"id\", \"name\", \"age\"}\n    \n    # Test metadata\n    metadata = default_catalog.describe_table(\"test_users\")\n    assert metadata[\"metadata\"][\"description\"] == \"Test user data\"\n    assert metadata[\"metadata\"][\"table_type\"] == \"function\"\n\ndef test_static_table_registration():\n    \"\"\"Test registering static table objects.\"\"\"\n    # Mock DeltaTable for testing\n    class MockDeltaTable:\n        def __init__(self, name):\n            self.name = name\n        \n        def read_data(self, **kwargs):\n            return pl.DataFrame({\n                \"event_id\": [1, 2, 3],\n                \"event_type\": [\"click\", \"view\", \"purchase\"],\n                \"timestamp\": [\"2025-01-01\", \"2025-01-02\", \"2025-01-03\"]\n            })\n    \n    mock_table = MockDeltaTable(\"events\")\n    register_static_table(\n        mock_table, \n        \"events\",\n        description=\"User events data\",\n        tags=[\"events\", \"analytics\"]\n    )\n    \n    # Verify registration\n    tables = default_catalog.list_tables()\n    assert \"events\" in tables\n    \n    # Test metadata\n    metadata = default_catalog.describe_table(\"events\")\n    assert metadata[\"metadata\"][\"description\"] == \"User events data\"\n    assert \"events\" in metadata[\"metadata\"][\"tags\"]\n\ndef test_catalog_table_listing():\n    \"\"\"Test table listing and filtering.\"\"\"\n    \n    @table(description=\"Function table\")\n    def func_table():\n        return pl.LazyFrame({\"id\": [1]})\n    \n    # Register a mock static table\n    class MockTable:\n        def read_data(self):\n            return pl.DataFrame({\"id\": [1]})\n    \n    register_static_table(MockTable(), \"static_table\", description=\"Static table\")\n    \n    # Test listing all tables\n    all_tables = default_catalog.list_tables()\n    assert \"func_table\" in all_tables\n    assert \"static_table\" in all_tables\n    \n    # Test filtering by type\n    function_tables = default_catalog.list_tables(TableType.FUNCTION)\n    assert \"func_table\" in function_tables\n    assert \"static_table\" not in function_tables\n\ndef test_catalog_metadata_export():\n    \"\"\"Test exporting catalog metadata.\"\"\"\n    \n    @table(description=\"Export test table\", tags=[\"test\"])\n    def export_test():\n        return pl.LazyFrame({\"col1\": [1, 2], \"col2\": [\"a\", \"b\"]})\n    \n    metadata = default_catalog.export_catalog_metadata()\n    \n    assert \"tables\" in metadata\n    assert \"export_test\" in metadata[\"tables\"]\n    assert metadata[\"tables\"][\"export_test\"][\"description\"] == \"Export test table\"\n    assert \"test\" in metadata[\"tables\"][\"export_test\"][\"tags\"]\n    assert metadata[\"total_tables\"] >= 1\n\ndef test_table_schema_inference():\n    \"\"\"Test schema inference from table data.\"\"\"\n    \n    @table(description=\"Schema test\")\n    def schema_table():\n        return pl.LazyFrame({\n            \"int_col\": [1, 2, 3],\n            \"str_col\": [\"a\", \"b\", \"c\"],\n            \"float_col\": [1.1, 2.2, 3.3],\n            \"bool_col\": [True, False, True]\n        })\n    \n    description = default_catalog.describe_table(\"schema_table\")\n    schema = description[\"inferred_schema\"]\n    \n    assert \"int_col\" in schema\n    assert \"str_col\" in schema\n    assert \"float_col\" in schema\n    assert \"bool_col\" in schema\n    assert description[\"column_count\"] == 4\n\nif __name__ == \"__main__\":\n    pytest.main([__file__, \"-v\"])\n```\n\n**Commands to test:**\n```bash\ncd neuralake/\npoetry run pytest tests/test_catalog_core.py -v\n```",
          "status": "done",
          "dependencies": [],
          "parentTaskId": 5
        }
      ]
    },
    {
      "id": 6,
      "title": "Implement Low-Latency 'Surgical Strike' Writer",
      "description": "Following the Neuralink three-process architecture, implement the core real-time ingestion mechanism with separate Writer, Compaction, and Vacuum processes using Rust for maximum performance.",
      "status": "in-progress",
      "dependencies": [
        4
      ],
      "priority": "high",
      "details": "1. Implement the Neuralink three-process architecture in Rust:\n   - **Writer Process:** Continuously appends small files to Delta tables with minimal latency\n   - **Compaction Process:** Scheduled process that merges small files into larger, read-optimized files\n   - **Vacuum Process:** Cleans up stale, unreferenced files beyond retention period\n2. Use AWS DynamoDB as the locking provider for delta-rs to enable safe concurrent operations\n3. Integrate with MinIO-based Delta Lake storage\n4. Implement inter-process communication and coordination\n5. Create intelligent compaction strategies (time-based, size-based)\n6. Add configuration options for tuning each process\n7. This is the primary mechanism for populating the lakehouse with low-latency requirements",
      "testStrategy": "1. Unit tests for each service component\n2. Integration tests for the complete write-compact-vacuum cycle\n3. Performance tests measuring write latency under various loads\n4. Stress tests with high-frequency small writes\n5. Test recovery scenarios after service failures\n6. Measure and validate file size distributions after compaction\n7. Test vacuum retention policies\n8. Test MinIO integration for all three processes\n9. Test DynamoDB locking mechanism\n10. Verify the solution addresses the 'small file problem'",
      "subtasks": [
        {
          "id": 1,
          "title": "Set up Rust project with verified crate versions",
          "description": "Create the rust-writer directory and Cargo.toml with the exact verified versions from research (no margin for error)",
          "details": "**CRITICAL: Use these EXACT verified versions from research 2025-06-20:**\n\n**Directory structure to create:**\n```\nneuralake/rust-writer/\nâ”œâ”€â”€ Cargo.toml\nâ”œâ”€â”€ src/\nâ”‚   â”œâ”€â”€ lib.rs\nâ”‚   â”œâ”€â”€ writer.rs\nâ”‚   â”œâ”€â”€ compactor.rs\nâ”‚   â”œâ”€â”€ vacuum.rs\nâ”‚   â”œâ”€â”€ config.rs\nâ”‚   â””â”€â”€ bin/\nâ”‚       â””â”€â”€ writer.rs\nâ””â”€â”€ tests/\n    â”œâ”€â”€ common/\n    â”‚   â””â”€â”€ mod.rs\n    â”œâ”€â”€ writer_unit.rs\n    â”œâ”€â”€ compaction_unit.rs\n    â”œâ”€â”€ vacuum_unit.rs\n    â”œâ”€â”€ locking_dynamodb.rs\n    â”œâ”€â”€ integration_e2e.rs\n    â””â”€â”€ perf_bench.rs\n```\n\n**File: `neuralake/rust-writer/Cargo.toml` (EXACT VERSIONS):**\n```toml\n[package]\nname = \"neuralake-writer\"\nversion = \"0.1.0\"\nedition = \"2021\"\nauthors = [\"Neuralake Team\"]\ndescription = \"High-performance, low-latency data writer for Neuralake\"\n\n[dependencies]\n# Core async runtime\ntokio = { version = \"1.45.1\", features = [\"rt-multi-thread\", \"macros\", \"time\", \"fs\"] }\n\n# Data processing - CRITICAL: use exact versions tested together\npolars = { version = \"0.49.1\", default-features = false, features = [\"lazy\", \"parquet\"] }\narrow = \"55.0\"  # First version with date-time kernel fix\n\n# Delta Lake - EXACT VERSION\ndeltalake = { version = \"0.26.2\", features = [\"s3\", \"dynamodb\"] }\nobject_store = \"0.12.2\"\n\n# AWS SDK - EXACT VERSIONS\naws-sdk-s3 = { version = \"1.93.0\", features = [\"rustls\"] }\naws-sdk-dynamodb = { version = \"1.80.0\", features = [\"rustls\"] }\n\n# Serialization\nserde = { version = \"1.0\", features = [\"derive\"] }\nserde_json = \"1.0\"\ntoml = \"0.8\"\n\n# Error handling\nanyhow = \"1.0\"\nthiserror = \"1.0\"\n\n# Logging\ntracing = \"0.1\"\ntracing-subscriber = \"0.3\"\n\n# Configuration\nclap = { version = \"4.4\", features = [\"derive\"] }\n\n# Utilities\nuuid = { version = \"1.6\", features = [\"v4\"] }\nchrono = { version = \"0.4\", features = [\"serde\"] }\n\n[dev-dependencies]\n# Testing infrastructure - EXACT VERSIONS\ntestcontainers = \"0.24.0\"  # NOTE: Research shows 0.24.0, not 0.15.x\ncriterion = { version = \"0.5.1\", features = [\"html_reports\"] }\nserde_json = \"1.0\"\nanyhow = \"1.0\"\n\n[[bench]]\nname = \"writer_bench\"\nharness = false\n\n[profile.release]\nlto = true\ncodegen-units = 1\npanic = \"abort\"\n\n[profile.bench]\ndebug = true\n```\n\n**Commands to execute:**\n```bash\n# Create directory structure\ncd neuralake/\nmkdir -p rust-writer/src/bin\nmkdir -p rust-writer/tests/common\ncd rust-writer/\n\n# Create Cargo.toml with exact versions above\n# CRITICAL: Copy the exact Cargo.toml content above\n\n# Verify versions are compatible\ncargo check\n\n# Install development tools\ncargo install cargo-watch\ncargo install cargo-tarpaulin\n\n# Set up TDD workflow\ncargo watch -x 'test --lib --tests'\n```\n\n**VALIDATION:** The research specifically states \"CI will fail fast if crates drift\" - these versions MUST be exact.",
          "status": "pending",
          "dependencies": [],
          "parentTaskId": 6
        }
      ]
    },
    {
      "id": 7,
      "title": "Develop Enhanced Testing Framework with Polars",
      "description": "Create a comprehensive testing framework using Polars as the standard DataFrame library, including unit tests, integration tests, and production verification scenarios.",
      "status": "pending",
      "dependencies": [
        2,
        3
      ],
      "priority": "high",
      "details": "1. Set up pytest as the primary testing framework with proper configuration\n2. Establish **Polars DataFrames** as the standard for all non-Spark data manipulation\n3. Create comprehensive unit tests for all neuralake modules using Polars for DataFrame operations\n4. Implement integration tests that cover the full data pipeline\n5. Add test fixtures and mock data generators for consistent testing\n6. Set up coverage reporting with pytest-cov\n7. Create production verification scenarios that can run in both local and production environments\n8. Implement performance and load testing capabilities with Polars benchmarks\n9. Add automated test execution to the CI/CD pipeline\n10. Include comparison testing between 'surgical strike' (Rust/Polars) and 'workhorse' (Spark) engines",
      "testStrategy": "1. Test framework setup and configuration validation\n2. Unit test coverage verification (target: >90%)\n3. Integration test execution against real MinIO instance\n4. Performance test baseline establishment with Polars benchmarks\n5. CI/CD integration testing\n6. Production verification scenario validation\n7. Load testing with various data sizes and query patterns\n8. Error injection and recovery testing\n9. Comparative testing between Polars and Spark implementations",
      "subtasks": [
        {
          "id": "7.1",
          "title": "Set up pytest framework and configuration",
          "status": "pending",
          "description": "Configure pytest with appropriate plugins and settings for the project"
        },
        {
          "id": "7.2",
          "title": "Create comprehensive unit test suite with Polars",
          "status": "pending",
          "description": "Develop unit tests for all core neuralake modules with high coverage using Polars for DataFrame operations"
        },
        {
          "id": "7.3",
          "title": "Implement integration test scenarios",
          "status": "pending",
          "description": "Create integration tests covering the complete data pipeline and MinIO interactions"
        },
        {
          "id": "7.4",
          "title": "Add coverage reporting and performance testing",
          "status": "pending",
          "description": "Set up coverage reporting with pytest-cov and implement performance test benchmarks comparing Polars and Spark"
        },
        {
          "id": 8.4,
          "title": "Implement comprehensive Polars-based testing framework",
          "description": "Create a standardized testing framework that uses Polars for all DataFrame operations, including data generation, validation, and performance testing",
          "details": "**File to create - `neuralake/src/testing_framework.py`:**\n```python\n\"\"\"\nComprehensive testing framework standardized on Polars.\nProvides utilities for data generation, validation, and performance testing.\n\"\"\"\nimport polars as pl\nimport numpy as np\nfrom typing import Dict, Any, List, Optional, Callable, Union\nfrom dataclasses import dataclass\nfrom pathlib import Path\nimport time\nfrom datetime import datetime, timedelta\nimport json\nimport logging\n\nlogger = logging.getLogger(__name__)\n\n@dataclass\nclass TestConfig:\n    \"\"\"Configuration for test data generation.\"\"\"\n    seed: int = 42\n    default_size: int = 1000\n    date_range_days: int = 365\n    string_length: int = 10\n    null_probability: float = 0.1\n\nclass PolarsTestData:\n    \"\"\"Polars-based test data generator.\"\"\"\n    \n    def __init__(self, config: TestConfig = None):\n        self.config = config or TestConfig()\n        np.random.seed(self.config.seed)\n    \n    def generate_neural_events(self, \n                              n_rows: int = None,\n                              n_channels: int = 256,\n                              start_time: datetime = None) -> pl.LazyFrame:\n        \"\"\"Generate neural event data similar to Neuralink format.\"\"\"\n        n_rows = n_rows or self.config.default_size\n        start_time = start_time or datetime.now() - timedelta(days=1)\n        \n        # Generate timestamps with microsecond precision\n        timestamps = [\n            start_time + timedelta(microseconds=i * 100 + np.random.randint(-50, 50))\n            for i in range(n_rows)\n        ]\n        \n        return pl.LazyFrame({\n            \"timestamp\": timestamps,\n            \"channel_id\": np.random.randint(0, n_channels, n_rows),\n            \"spike_amplitude\": np.random.exponential(2.0, n_rows),\n            \"spike_width_us\": np.random.normal(500, 100, n_rows),\n            \"quality_score\": np.random.beta(5, 2, n_rows),\n            \"session_id\": [f\"session_{np.random.randint(1, 10)}\" for _ in range(n_rows)],\n            \"electrode_impedance\": np.random.lognormal(3, 0.5, n_rows),\n            \"is_artifact\": np.random.choice([True, False], n_rows, p=[0.05, 0.95])\n        })\n    \n    def generate_user_behavior(self, n_rows: int = None) -> pl.LazyFrame:\n        \"\"\"Generate user behavior event data.\"\"\"\n        n_rows = n_rows or self.config.default_size\n        \n        return pl.LazyFrame({\n            \"user_id\": [f\"user_{np.random.randint(1, 1000)}\" for _ in range(n_rows)],\n            \"event_type\": np.random.choice(\n                [\"click\", \"view\", \"purchase\", \"scroll\", \"hover\"], \n                n_rows, \n                p=[0.3, 0.4, 0.05, 0.2, 0.05]\n            ),\n            \"timestamp\": [\n                datetime.now() - timedelta(seconds=np.random.randint(0, 86400))\n                for _ in range(n_rows)\n            ],\n            \"page_path\": [f\"/page/{np.random.randint(1, 100)}\" for _ in range(n_rows)],\n            \"session_duration_ms\": np.random.exponential(30000, n_rows),\n            \"device_type\": np.random.choice([\"mobile\", \"desktop\", \"tablet\"], n_rows),\n            \"revenue\": np.where(\n                np.random.choice([\"click\", \"view\", \"purchase\", \"scroll\", \"hover\"], n_rows) == \"purchase\",\n                np.random.exponential(50), \n                0\n            )\n        })\n    \n    def generate_time_series(self, \n                           n_rows: int = None,\n                           freq_seconds: int = 1,\n                           n_series: int = 5) -> pl.LazyFrame:\n        \"\"\"Generate multi-variate time series data.\"\"\"\n        n_rows = n_rows or self.config.default_size\n        \n        # Generate base time series\n        timestamps = [\n            datetime.now() - timedelta(seconds=(n_rows - i) * freq_seconds)\n            for i in range(n_rows)\n        ]\n        \n        data = {\"timestamp\": timestamps}\n        \n        # Generate multiple time series with different patterns\n        for i in range(n_series):\n            # Add trend, seasonality, and noise\n            trend = np.linspace(0, 10, n_rows)\n            seasonal = 5 * np.sin(2 * np.pi * np.arange(n_rows) / (86400 / freq_seconds))  # Daily pattern\n            noise = np.random.normal(0, 1, n_rows)\n            \n            data[f\"metric_{i}\"] = trend + seasonal + noise\n        \n        return pl.LazyFrame(data)\n    \n    def generate_hierarchical_data(self, n_rows: int = None) -> pl.LazyFrame:\n        \"\"\"Generate hierarchical data for testing aggregations.\"\"\"\n        n_rows = n_rows or self.config.default_size\n        \n        return pl.LazyFrame({\n            \"country\": np.random.choice([\"US\", \"UK\", \"DE\", \"FR\", \"JP\"], n_rows),\n            \"region\": [f\"region_{np.random.randint(1, 10)}\" for _ in range(n_rows)],\n            \"city\": [f\"city_{np.random.randint(1, 100)}\" for _ in range(n_rows)],\n            \"category\": np.random.choice([\"A\", \"B\", \"C\", \"D\"], n_rows),\n            \"subcategory\": [f\"sub_{np.random.randint(1, 20)}\" for _ in range(n_rows)],\n            \"value\": np.random.exponential(100, n_rows),\n            \"count\": np.random.poisson(10, n_rows),\n            \"date\": [\n                datetime.now().date() - timedelta(days=np.random.randint(0, 365))\n                for _ in range(n_rows)\n            ]\n        })\n\nclass PolarsValidator:\n    \"\"\"Validation utilities for Polars DataFrames.\"\"\"\n    \n    @staticmethod\n    def validate_schema(df: pl.LazyFrame, expected_schema: Dict[str, pl.DataType]) -> Dict[str, Any]:\n        \"\"\"Validate DataFrame schema against expected types.\"\"\"\n        try:\n            # Collect a single row to get schema\n            sample = df.limit(1).collect()\n            actual_schema = dict(zip(sample.columns, sample.dtypes))\n            \n            results = {\n                \"valid\": True,\n                \"missing_columns\": [],\n                \"extra_columns\": [],\n                \"type_mismatches\": {}\n            }\n            \n            # Check for missing columns\n            expected_cols = set(expected_schema.keys())\n            actual_cols = set(actual_schema.keys())\n            \n            results[\"missing_columns\"] = list(expected_cols - actual_cols)\n            results[\"extra_columns\"] = list(actual_cols - expected_cols)\n            \n            # Check type mismatches\n            for col in expected_cols & actual_cols:\n                if actual_schema[col] != expected_schema[col]:\n                    results[\"type_mismatches\"][col] = {\n                        \"expected\": str(expected_schema[col]),\n                        \"actual\": str(actual_schema[col])\n                    }\n            \n            results[\"valid\"] = (\n                len(results[\"missing_columns\"]) == 0 and\n                len(results[\"type_mismatches\"]) == 0\n            )\n            \n            return results\n            \n        except Exception as e:\n            return {\"valid\": False, \"error\": str(e)}\n    \n    @staticmethod\n    def validate_data_quality(df: pl.LazyFrame) -> Dict[str, Any]:\n        \"\"\"Perform comprehensive data quality checks.\"\"\"\n        try:\n            # Collect summary statistics\n            collected = df.collect()\n            \n            results = {\n                \"total_rows\": collected.height,\n                \"total_columns\": collected.width,\n                \"null_counts\": {},\n                \"duplicate_rows\": 0,\n                \"numeric_ranges\": {},\n                \"string_stats\": {}\n            }\n            \n            # Check null values\n            for col in collected.columns:\n                null_count = collected[col].null_count()\n                results[\"null_counts\"][col] = {\n                    \"count\": null_count,\n                    \"percentage\": null_count / collected.height * 100\n                }\n            \n            # Check for duplicate rows\n            results[\"duplicate_rows\"] = collected.height - collected.unique().height\n            \n            # Analyze numeric columns\n            for col in collected.columns:\n                dtype = collected[col].dtype\n                if dtype in [pl.Int32, pl.Int64, pl.Float32, pl.Float64]:\n                    col_data = collected[col].drop_nulls()\n                    if len(col_data) > 0:\n                        results[\"numeric_ranges\"][col] = {\n                            \"min\": float(col_data.min()),\n                            \"max\": float(col_data.max()),\n                            \"mean\": float(col_data.mean()),\n                            \"std\": float(col_data.std()) if len(col_data) > 1 else 0.0\n                        }\n                \n                elif dtype == pl.Utf8:\n                    col_data = collected[col].drop_nulls()\n                    if len(col_data) > 0:\n                        lengths = col_data.str.len_chars()\n                        results[\"string_stats\"][col] = {\n                            \"min_length\": int(lengths.min()),\n                            \"max_length\": int(lengths.max()),\n                            \"avg_length\": float(lengths.mean()),\n                            \"unique_values\": col_data.n_unique()\n                        }\n            \n            return results\n            \n        except Exception as e:\n            return {\"error\": str(e)}\n\nclass PolarsPerformanceTester:\n    \"\"\"Performance testing utilities for Polars operations.\"\"\"\n    \n    def __init__(self):\n        self.results = []\n    \n    def time_operation(self, \n                      operation: Callable[[pl.LazyFrame], pl.LazyFrame],\n                      data: pl.LazyFrame,\n                      name: str = \"operation\",\n                      collect: bool = True) -> Dict[str, Any]:\n        \"\"\"Time a Polars operation and return performance metrics.\"\"\"\n        \n        # Warmup run\n        try:\n            if collect:\n                operation(data).collect()\n            else:\n                operation(data)\n        except Exception as e:\n            return {\"name\": name, \"error\": f\"Warmup failed: {e}\"}\n        \n        # Timed runs\n        times = []\n        for _ in range(3):  # Run 3 times for average\n            start_time = time.perf_counter()\n            try:\n                if collect:\n                    result = operation(data).collect()\n                else:\n                    result = operation(data)\n                end_time = time.perf_counter()\n                times.append(end_time - start_time)\n            except Exception as e:\n                return {\"name\": name, \"error\": f\"Operation failed: {e}\"}\n        \n        # Calculate statistics\n        avg_time = np.mean(times)\n        std_time = np.std(times)\n        \n        # Memory estimation (rough)\n        if collect and 'result' in locals():\n            estimated_memory_mb = result.estimated_size(\"mb\")\n        else:\n            estimated_memory_mb = None\n        \n        perf_result = {\n            \"name\": name,\n            \"avg_time_seconds\": avg_time,\n            \"std_time_seconds\": std_time,\n            \"min_time_seconds\": min(times),\n            \"max_time_seconds\": max(times),\n            \"estimated_memory_mb\": estimated_memory_mb,\n            \"all_times\": times\n        }\n        \n        self.results.append(perf_result)\n        return perf_result\n    \n    def benchmark_common_operations(self, df: pl.LazyFrame) -> List[Dict[str, Any]]:\n        \"\"\"Benchmark common DataFrame operations.\"\"\"\n        \n        operations = [\n            (\"filter\", lambda x: x.filter(pl.col(\"timestamp\") > datetime.now() - timedelta(hours=1))),\n            (\"group_by\", lambda x: x.group_by(\"category\").agg(pl.col(\"value\").sum())),\n            (\"sort\", lambda x: x.sort(\"timestamp\")),\n            (\"join_self\", lambda x: x.join(x, on=\"timestamp\", how=\"inner\")),\n            (\"window_function\", lambda x: x.with_columns(\n                pl.col(\"value\").sum().over(\"category\").alias(\"category_total\")\n            )),\n            (\"string_operations\", lambda x: x.with_columns(\n                pl.col(\"category\").str.upper().alias(\"category_upper\")\n            ) if \"category\" in x.columns else x),\n        ]\n        \n        results = []\n        for name, operation in operations:\n            try:\n                result = self.time_operation(operation, df, name)\n                results.append(result)\n                logger.info(f\"Benchmarked {name}: {result.get('avg_time_seconds', 'N/A'):.4f}s\")\n            except Exception as e:\n                logger.error(f\"Failed to benchmark {name}: {e}\")\n                results.append({\"name\": name, \"error\": str(e)})\n        \n        return results\n    \n    def export_results(self, output_path: Path) -> None:\n        \"\"\"Export performance results to JSON.\"\"\"\n        output_path.parent.mkdir(parents=True, exist_ok=True)\n        \n        export_data = {\n            \"benchmark_timestamp\": datetime.now().isoformat(),\n            \"results\": self.results,\n            \"summary\": {\n                \"total_operations\": len(self.results),\n                \"avg_time\": np.mean([r.get(\"avg_time_seconds\", 0) for r in self.results if \"error\" not in r]),\n                \"fastest_operation\": min(self.results, key=lambda x: x.get(\"avg_time_seconds\", float(\"inf\")), default={}),\n                \"slowest_operation\": max(self.results, key=lambda x: x.get(\"avg_time_seconds\", 0), default={})\n            }\n        }\n        \n        with open(output_path, 'w') as f:\n            json.dump(export_data, f, indent=2, default=str)\n\n# Convenience functions\ndef create_test_data() -> PolarsTestData:\n    \"\"\"Create a test data generator with default config.\"\"\"\n    return PolarsTestData()\n\ndef validate_dataframe(df: pl.LazyFrame, expected_schema: Dict[str, pl.DataType] = None) -> Dict[str, Any]:\n    \"\"\"Quick validation of a DataFrame.\"\"\"\n    validator = PolarsValidator()\n    \n    results = {}\n    \n    if expected_schema:\n        results[\"schema_validation\"] = validator.validate_schema(df, expected_schema)\n    \n    results[\"data_quality\"] = validator.validate_data_quality(df)\n    \n    return results\n\ndef benchmark_dataframe_operations(df: pl.LazyFrame) -> List[Dict[str, Any]]:\n    \"\"\"Quick benchmark of common operations on a DataFrame.\"\"\"\n    tester = PolarsPerformanceTester()\n    return tester.benchmark_common_operations(df)\n```\n\n**File to create - `neuralake/tests/test_polars_framework.py`:**\n```python\n\"\"\"\nTests for the Polars-based testing framework.\n\"\"\"\nimport pytest\nimport polars as pl\nfrom datetime import datetime, timedelta\nfrom pathlib import Path\nimport tempfile\n\nfrom src.testing_framework import (\n    PolarsTestData, PolarsValidator, PolarsPerformanceTester,\n    TestConfig, create_test_data, validate_dataframe, benchmark_dataframe_operations\n)\n\nclass TestPolarsTestData:\n    \n    def test_neural_events_generation(self):\n        \"\"\"Test neural events data generation.\"\"\"\n        generator = PolarsTestData()\n        df = generator.generate_neural_events(n_rows=100)\n        \n        collected = df.collect()\n        assert collected.height == 100\n        \n        expected_columns = {\n            \"timestamp\", \"channel_id\", \"spike_amplitude\", \"spike_width_us\",\n            \"quality_score\", \"session_id\", \"electrode_impedance\", \"is_artifact\"\n        }\n        assert set(collected.columns) == expected_columns\n        \n        # Validate data types\n        assert collected[\"timestamp\"].dtype == pl.Datetime\n        assert collected[\"channel_id\"].dtype in [pl.Int32, pl.Int64]\n        assert collected[\"spike_amplitude\"].dtype in [pl.Float32, pl.Float64]\n        assert collected[\"is_artifact\"].dtype == pl.Boolean\n    \n    def test_user_behavior_generation(self):\n        \"\"\"Test user behavior data generation.\"\"\"\n        generator = PolarsTestData()\n        df = generator.generate_user_behavior(n_rows=50)\n        \n        collected = df.collect()\n        assert collected.height == 50\n        \n        # Check that event types are from expected set\n        event_types = set(collected[\"event_type\"].unique())\n        expected_events = {\"click\", \"view\", \"purchase\", \"scroll\", \"hover\"}\n        assert event_types.issubset(expected_events)\n        \n        # Check revenue logic (only purchases should have revenue > 0)\n        purchases = collected.filter(pl.col(\"event_type\") == \"purchase\")\n        non_purchases = collected.filter(pl.col(\"event_type\") != \"purchase\")\n        \n        if purchases.height > 0:\n            assert purchases[\"revenue\"].min() >= 0\n        if non_purchases.height > 0:\n            assert non_purchases[\"revenue\"].max() == 0\n\nclass TestPolarsValidator:\n    \n    def test_schema_validation(self):\n        \"\"\"Test schema validation functionality.\"\"\"\n        df = pl.LazyFrame({\n            \"id\": [1, 2, 3],\n            \"name\": [\"Alice\", \"Bob\", \"Charlie\"],\n            \"age\": [25, 30, 35]\n        })\n        \n        # Valid schema\n        expected_schema = {\n            \"id\": pl.Int64,\n            \"name\": pl.Utf8,\n            \"age\": pl.Int64\n        }\n        \n        validator = PolarsValidator()\n        result = validator.validate_schema(df, expected_schema)\n        \n        assert result[\"valid\"] is True\n        assert len(result[\"missing_columns\"]) == 0\n        assert len(result[\"type_mismatches\"]) == 0\n    \n    def test_schema_validation_failures(self):\n        \"\"\"Test schema validation with mismatches.\"\"\"\n        df = pl.LazyFrame({\n            \"id\": [1, 2, 3],\n            \"name\": [\"Alice\", \"Bob\", \"Charlie\"]\n            # Missing \"age\" column\n        })\n        \n        expected_schema = {\n            \"id\": pl.Int64,\n            \"name\": pl.Utf8,\n            \"age\": pl.Int64,  # This column is missing\n            # Missing \"extra\" column definition\n        }\n        \n        validator = PolarsValidator()\n        result = validator.validate_schema(df, expected_schema)\n        \n        assert result[\"valid\"] is False\n        assert \"age\" in result[\"missing_columns\"]\n    \n    def test_data_quality_validation(self):\n        \"\"\"Test data quality validation.\"\"\"\n        df = pl.LazyFrame({\n            \"id\": [1, 2, 3, 3, None],  # Has duplicate and null\n            \"value\": [10.5, 20.0, None, 15.5, 25.0],  # Has null\n            \"category\": [\"A\", \"B\", \"A\", \"C\", \"B\"]  # No nulls\n        })\n        \n        validator = PolarsValidator()\n        result = validator.validate_data_quality(df)\n        \n        assert result[\"total_rows\"] == 5\n        assert result[\"total_columns\"] == 3\n        assert result[\"duplicate_rows\"] == 1  # One duplicate row\n        \n        # Check null counts\n        assert result[\"null_counts\"][\"id\"][\"count\"] == 1\n        assert result[\"null_counts\"][\"value\"][\"count\"] == 1\n        assert result[\"null_counts\"][\"category\"][\"count\"] == 0\n\nclass TestPolarsPerformanceTester:\n    \n    def test_performance_timing(self):\n        \"\"\"Test performance timing functionality.\"\"\"\n        df = pl.LazyFrame({\n            \"id\": range(1000),\n            \"value\": range(1000)\n        })\n        \n        tester = PolarsPerformanceTester()\n        \n        # Test a simple operation\n        result = tester.time_operation(\n            lambda x: x.filter(pl.col(\"id\") > 500),\n            df,\n            \"filter_operation\"\n        )\n        \n        assert \"avg_time_seconds\" in result\n        assert \"name\" in result\n        assert result[\"name\"] == \"filter_operation\"\n        assert result[\"avg_time_seconds\"] > 0\n    \n    def test_benchmark_operations(self):\n        \"\"\"Test benchmarking common operations.\"\"\"\n        generator = PolarsTestData()\n        df = generator.generate_time_series(n_rows=1000)\n        \n        tester = PolarsPerformanceTester()\n        results = tester.benchmark_common_operations(df)\n        \n        assert len(results) > 0\n        \n        # Check that we have some successful benchmarks\n        successful_benchmarks = [r for r in results if \"error\" not in r]\n        assert len(successful_benchmarks) > 0\n        \n        for result in successful_benchmarks:\n            assert \"avg_time_seconds\" in result\n            assert result[\"avg_time_seconds\"] >= 0\n\nclass TestConvenienceFunctions:\n    \n    def test_create_test_data(self):\n        \"\"\"Test the convenience function for creating test data.\"\"\"\n        generator = create_test_data()\n        assert isinstance(generator, PolarsTestData)\n        \n        df = generator.generate_neural_events(n_rows=10)\n        collected = df.collect()\n        assert collected.height == 10\n    \n    def test_validate_dataframe(self):\n        \"\"\"Test the convenience validation function.\"\"\"\n        df = pl.LazyFrame({\n            \"id\": [1, 2, 3],\n            \"name\": [\"A\", \"B\", \"C\"]\n        })\n        \n        expected_schema = {\n            \"id\": pl.Int64,\n            \"name\": pl.Utf8\n        }\n        \n        result = validate_dataframe(df, expected_schema)\n        \n        assert \"schema_validation\" in result\n        assert \"data_quality\" in result\n        assert result[\"schema_validation\"][\"valid\"] is True\n    \n    def test_benchmark_dataframe_operations(self):\n        \"\"\"Test the convenience benchmarking function.\"\"\"\n        df = pl.LazyFrame({\n            \"category\": [\"A\", \"B\", \"A\", \"C\"] * 100,\n            \"value\": range(400),\n            \"timestamp\": [datetime.now() - timedelta(minutes=i) for i in range(400)]\n        })\n        \n        results = benchmark_dataframe_operations(df)\n        assert len(results) > 0\n\nif __name__ == \"__main__\":\n    pytest.main([__file__, \"-v\"])\n```\n\n**Commands to test the framework:**\n```bash\ncd neuralake/\n\n# Run Polars framework tests\npoetry run pytest tests/test_polars_framework.py -v\n\n# Run performance benchmarks\npoetry run python -c \"\nfrom src.testing_framework import create_test_data, benchmark_dataframe_operations\ngenerator = create_test_data()\ndf = generator.generate_neural_events(n_rows=10000)\nresults = benchmark_dataframe_operations(df)\nprint('Benchmark results:', results)\n\"\n```",
          "status": "pending",
          "dependencies": [],
          "parentTaskId": 7
        }
      ]
    },
    {
      "id": 8,
      "title": "Enhance Sample Data Generation with Polars",
      "description": "Refactor scripts/create_sample_data.py to support generating larger, more complex datasets using Polars for data generation and manipulation.",
      "status": "pending",
      "dependencies": [
        1
      ],
      "priority": "medium",
      "details": "1. Design a flexible schema definition format (JSON/YAML)\n2. Use **Polars** for all data generation and manipulation operations\n3. Implement data generators for common data types using Faker and NumPy\n4. Support statistical distributions via NumPy's distribution functions\n5. Add correlation capabilities between fields\n6. Implement parallel generation for performance\n7. Support direct parallel writing to MinIO in Parquet format\n8. Configure the utility to create many small files rather than few large ones\n9. Add progress reporting and resumability for large generations",
      "testStrategy": "1. Unit tests for each data generator type\n2. Validation tests to ensure generated data matches specified distributions\n3. Performance tests measuring generation speed with Polars\n4. Memory usage monitoring during large dataset generation\n5. End-to-end test generating a small dataset and validating schema compliance\n6. Test parallel generation with different thread counts\n7. Test parallel writes to MinIO with various file size configurations\n8. Benchmark Polars performance vs other DataFrame libraries",
      "subtasks": []
    },
    {
      "id": 9,
      "title": "Set Up Apache Kafka for Real-time Ingestion",
      "description": "Configure Kafka infrastructure using Docker, configure topics and partitions, implement Kafka clients that integrate with the low-latency writer system for continuous data flow into Delta tables.",
      "status": "pending",
      "dependencies": [
        6
      ],
      "priority": "medium",
      "details": "1. Set up Kafka infrastructure using Docker containers\n2. Configure topics and partitions for neural data streams\n3. Implement Kafka producers for data ingestion\n4. Create Kafka consumers that integrate with the low-latency writer\n5. Configure serialization/deserialization for efficient data transfer\n6. Implement error handling and retry mechanisms\n7. Add monitoring and alerting for Kafka cluster health\n8. Document Kafka setup and configuration procedures",
      "testStrategy": "1. Test Kafka container startup and configuration\n2. Verify topic creation and partition allocation\n3. Test producer throughput and latency\n4. Test consumer integration with writer system\n5. Test error handling and recovery scenarios\n6. Performance testing with high-volume data streams\n7. Test monitoring and alerting functionality\n8. End-to-end testing of Kafka â†’ Writer â†’ Delta pipeline",
      "subtasks": []
    },
    {
      "id": 10,
      "title": "Implement Auto-Generated SQL API via ROAPI",
      "description": "Following the Neuralink model, implement ROAPI (Read-Only API) using Apache DataFusion as the query engine. Auto-generate HTTP APIs that accept SQL queries from the Code as a Catalog table definitions.",
      "status": "pending",
      "dependencies": [
        5
      ],
      "priority": "medium",
      "details": "1. Set up ROAPI with Apache DataFusion as the query engine\n2. Configure ROAPI to read from Delta Lake tables on MinIO\n3. Implement automatic API endpoint generation from catalog definitions\n4. Support SQL query execution via HTTP endpoints\n5. Add query optimization and caching capabilities\n6. Implement authentication and rate limiting\n7. Add comprehensive API documentation generation\n8. Ensure zero-maintenance API updates when tables change",
      "testStrategy": "1. Test ROAPI setup and DataFusion integration\n2. Verify automatic endpoint generation from catalog\n3. Test SQL query execution via HTTP APIs\n4. Performance testing with complex queries\n5. Test authentication and security features\n6. Test API documentation generation\n7. Test automatic updates when catalog changes\n8. Load testing with concurrent API requests",
      "subtasks": []
    },
    {
      "id": 11,
      "title": "Create Performance Benchmarking Framework",
      "description": "Develop a suite of benchmark tests to measure Polars/DataFusion query performance against scaled datasets on MinIO, comparing against Spark performance for dual-engine validation.",
      "status": "pending",
      "dependencies": [
        7,
        10
      ],
      "priority": "medium",
      "details": "1. Design benchmark suite with standard query patterns (scan, filter, join, aggregate)\n2. Implement benchmarks using both Polars/DataFusion ('surgical strike') and Spark ('workhorse')\n3. Create scalable test datasets of varying sizes\n4. Measure and compare query performance across engines\n5. Generate performance reports and visualizations\n6. Add automated benchmark execution to CI/CD pipeline\n7. Document performance characteristics and recommendations\n8. Establish performance regression detection",
      "testStrategy": "1. Validate benchmark accuracy and repeatability\n2. Test against datasets of various sizes\n3. Verify performance measurement accuracy\n4. Test both single-node and distributed scenarios\n5. Validate performance report generation\n6. Test CI/CD integration for automated benchmarking\n7. Test regression detection capabilities\n8. Compare results against industry benchmarks",
      "subtasks": []
    },
    {
      "id": 12,
      "title": "Set Up Containerized Apache Spark Environment",
      "description": "Configure a containerized Apache Spark environment that can connect to the same Delta Lake on MinIO, providing the 'workhorse' complement to the 'surgical strike' Rust stack for large-scale ELT operations.",
      "status": "pending",
      "dependencies": [
        4
      ],
      "priority": "medium",
      "details": "1. Set up containerized Spark cluster using Docker\n2. Configure Spark to connect to Delta Lake on MinIO\n3. Install and configure delta-spark libraries\n4. Set up Spark UI and monitoring\n5. Configure resource allocation and tuning\n6. Implement Spark job submission workflows\n7. Add logging and error handling\n8. Document Spark setup and usage procedures",
      "testStrategy": "1. Test Spark cluster startup and configuration\n2. Verify Delta Lake connectivity and operations\n3. Test read/write operations with MinIO\n4. Performance testing with large datasets\n5. Test resource allocation and scaling\n6. Test job submission and monitoring\n7. Test error handling and recovery\n8. Integration testing with existing pipeline",
      "subtasks": []
    },
    {
      "id": 13,
      "title": "Develop Sample Large-scale ELT Job in Spark",
      "description": "Create a sample large-scale ELT job in Spark that showcases the 'workhorse' part of the dual-engine philosophy, reading from Delta tables created by the low-latency writer and performing complex transformations.",
      "status": "pending",
      "dependencies": [
        12
      ],
      "priority": "medium",
      "details": "1. Design a representative ELT workflow using Spark\n2. Implement data transformation logic for complex analytics\n3. Read from Delta tables created by the low-latency writer\n4. Perform aggregations, joins, and complex computations\n5. Write results back to new Delta tables\n6. Add data quality checks and validation\n7. Implement error handling and recovery\n8. Document the ELT workflow and patterns",
      "testStrategy": "1. Test ELT job execution with sample data\n2. Verify data transformation accuracy\n3. Test performance with large datasets\n4. Test data quality checks and validation\n5. Test error handling and recovery scenarios\n6. Test integration with Delta Lake tables\n7. Performance comparison with Polars/DataFusion\n8. End-to-end pipeline testing",
      "subtasks": []
    },
    {
      "id": 14,
      "title": "Real-time Ingestion Pipeline Integration",
      "description": "Build a complete pipeline that connects Kafka â†’ Low-Latency Writer â†’ Delta Tables â†’ ROAPI, demonstrating the full real-time data flow with both hot path (real-time queries) and cold path (Spark ELT) capabilities.",
      "status": "pending",
      "dependencies": [
        9,
        10,
        13
      ],
      "priority": "high",
      "details": "1. Integrate all pipeline components into a cohesive system\n2. Implement end-to-end data flow from Kafka to ROAPI\n3. Configure hot path for real-time queries via ROAPI\n4. Configure cold path for batch ELT via Spark\n5. Add pipeline monitoring and observability\n6. Implement data lineage tracking\n7. Add automated testing for the complete pipeline\n8. Document the integrated architecture and operations",
      "testStrategy": "1. End-to-end pipeline testing with real data\n2. Test hot path latency and performance\n3. Test cold path throughput and accuracy\n4. Test pipeline resilience and error recovery\n5. Test monitoring and alerting systems\n6. Test data lineage tracking\n7. Load testing with high-volume data streams\n8. Disaster recovery and failover testing",
      "subtasks": []
    },
    {
      "id": 15,
      "title": "Enhanced Code-as-Catalog Features",
      "description": "Extend the catalog system with support for complex data sources (CSV files, JDBC connections), materialized views stored as Delta tables, and integrated data quality checks.",
      "status": "pending",
      "dependencies": [
        5,
        14
      ],
      "priority": "medium",
      "details": "1. Add support for CSV files as catalog data sources\n2. Implement JDBC connection support for external databases\n3. Create materialized view definitions stored as Delta tables\n4. Integrate data quality checks within the catalog framework\n5. Add data lineage tracking for complex transformations\n6. Implement catalog versioning and change management\n7. Add advanced metadata management capabilities\n8. Extend static site generation for new features",
      "testStrategy": "1. Test CSV file integration and metadata extraction\n2. Test JDBC connection and data access\n3. Test materialized view creation and management\n4. Test data quality check integration\n5. Test lineage tracking for complex scenarios\n6. Test catalog versioning and change management\n7. Test extended static site generation\n8. Integration testing with existing catalog features",
      "subtasks": []
    },
    {
      "id": 16,
      "title": "Advanced Monitoring and Data Governance",
      "description": "Implement comprehensive metrics collection, data lineage tracking, and automated compliance reporting. Integrate with the catalog system to provide governance metadata alongside technical metadata.",
      "status": "pending",
      "dependencies": [
        15
      ],
      "priority": "medium",
      "details": "1. Implement comprehensive metrics collection across all components\n2. Add data lineage tracking throughout the pipeline\n3. Create automated compliance reporting capabilities\n4. Integrate governance metadata with the catalog system\n5. Add data quality monitoring and alerting\n6. Implement access control and audit logging\n7. Create governance dashboards and reports\n8. Add automated policy enforcement capabilities",
      "testStrategy": "1. Test metrics collection and aggregation\n2. Test data lineage tracking accuracy\n3. Test compliance reporting generation\n4. Test governance metadata integration\n5. Test data quality monitoring and alerts\n6. Test access control enforcement\n7. Test dashboard functionality and reports\n8. Test policy enforcement automation",
      "subtasks": []
    },
    {
      "id": 17,
      "title": "End-to-End Integration and Documentation",
      "description": "Integrate all components into a cohesive system with unified deployment configuration, comprehensive documentation, and end-to-end examples showcasing the complete data flow from ingestion through the dual-engine query capabilities.",
      "status": "pending",
      "dependencies": [
        16
      ],
      "priority": "high",
      "details": "1. Create unified deployment configuration for all components\n2. Implement comprehensive system documentation\n3. Create end-to-end examples and tutorials\n4. Add system health checks and monitoring\n5. Implement automated deployment and scaling\n6. Create troubleshooting guides and runbooks\n7. Add performance tuning documentation\n8. Create user guides for different personas (developers, analysts, operators)",
      "testStrategy": "1. Test unified deployment across environments\n2. Validate documentation accuracy and completeness\n3. Test end-to-end examples and tutorials\n4. Test system health checks and monitoring\n5. Test automated deployment and scaling\n6. Validate troubleshooting guides\n7. Test performance tuning recommendations\n8. User acceptance testing with different personas",
      "subtasks": []
    }
  ],
  "metadata": {
    "created": "2025-06-19T13:06:06.120Z",
    "updated": "2025-06-20T06:57:27.328Z",
    "description": "Tasks aligned with Neuralink engineering blueprint: foundational improvements followed by dual-engine architecture implementation"
  }
}